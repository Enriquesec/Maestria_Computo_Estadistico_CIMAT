\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{xspace}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}
\usepackage{framed}



\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\title{Modelos no paramétricos y de regresión 2018-1}
\author{Tarea examen: pruebas binomiales y tablas de contingencia}
\date{Fecha de entrega: 08/01/2017}
\setlength{\parindent}{0in}
\spanishdecimal{.}

\newcommand{\X}{\mathbb{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\xbarn}{\bar{x}_n}
\newcommand{\ybarn}{\bar{y}_n}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\llaves}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\barra}{\,\vert\,}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mE}{\mathbb{E}}
\newcommand{\mR}{\mathbb{R}}
\newcommand{\mJ}{\mathbf{J}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\mA}{\mathbf{A}}
\newcommand{\unos}{\boldsymbol{1}}
\newcommand{\xbarnv}{\bar{\mathbf{x}}_n}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\muv}{\boldsymbol{\mu}}
\newcommand{\mcov}{\boldsymbol{\Sigma}}
\newcommand{\vbet}{\boldsymbol{\beta}}
\newcommand{\veps}{\boldsymbol{\epsilon}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\ceros}{\boldsymbol{0}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\avec}{\mathbf{a}}
\newcommand{\res}{\textbf{RESPUESTA}\\}

\newcommand{\defi}[3]{\textbf{Definición:#3}}
\newcommand{\fin}{$\blacksquare.$}
\newcommand{\finf}{\blacksquare.}
\newcommand{\tr}{\text{tr}}
\newcommand*{\temp}{\multicolumn{1}{r|}{}}

\newcommand{\grstep}[2][\relax]{%
   \ensuremath{\mathrel{
       {\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
                                     \begin{subarray}{l} #1 \end{subarray}}}}}}
\newcommand{\swap}{\leftrightarrow}

\newcommand{\gen}{\text{gen}}
\newtheorem{thmt}{Teorema:}
\newtheorem{thmd}{Definición:}
\newtheorem{thml}{Lema:}

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{c}
\textbf{Maestría en Computo Estadístico}\\
\textbf{Álgebra Matricial} \\
\textbf{Tarea 3}\\
\today \\
\emph{Enrique Santibáñez Cortés}\\
Repositorio de Git: \href{https://github.com/Enriquesec/Algebra_matricial/tree/master/tareas/Tarea_5}{Tarea 5, AM}.
\end{tabular}
\end{table}
Todos los cálculos deben ser a mano.
\begin{enumerate}

%Problema 1
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Encuentre la descomposición $LDU$ de la matriz
\begin{align*}
\begin{pmatrix}
1 & 2 & 4\\
3 & 8 & 14\\
2 & 6 & 13
\end{pmatrix}.
\end{align*}

\res
Recordemos la definición de la descomposición $LDU$.
\begin{framed}
    \begin{thmd} \label{descompotition_LDU}
    (Definición vista en clase) Sea $A$ no singular de tamaño $n\times n$. Entonces $A=LDU$ donde L es $n\times n$ es una matriz triangular inferior con unos en la diagonal, $D$ es $n\times n$ es una matriz diagonal con elementos diagonales no cero y $U$ es $n\times n$ es una matriz triangular superior con unos en la diagonal.
    \end{thmd}
\end{framed} 

Por la definición \ref{descompotition_LDU}, primero busquemos la matriz escalonada para ello ocupemos eliminicación gaussiana:
\begin{align*}
\begin{pmatrix}
1 & 2 & 4\\
3 & 8 & 14\\
2 & 6 & 13
\end{pmatrix}%
\grstep[R3 \rightarrow R_3-2R_1]{R_2 \rightarrow R_2-3R_1}
%
\begin{pmatrix}
1 & 2 & 4\\
0 & 2 & 2\\
0 & 2 & 5
\end{pmatrix}%
\grstep[]{R_3 \rightarrow R_3-R_2}
%
\begin{pmatrix}
1 & 2 & 4\\
0 & 2 & 2\\
0 & 0 & 3
\end{pmatrix}.
\end{align*}
Entonces, de la matriz resultante podemos decir que
\begin{align*}
D=\begin{pmatrix}
1 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 3
\end{pmatrix}\ \ \text{y} \ \ U=\begin{pmatrix}
1 & a & b\\
0 & 1 & c\\
0 & 0 & 1
\end{pmatrix}\ \text{tal que}\ \begin{pmatrix}
1 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 3
\end{pmatrix}\begin{pmatrix}
1 & a & b\\
0 & 1 & c\\
0 & 0 & 1
\end{pmatrix}=\begin{pmatrix}
1 & 2 & 4\\
0 & 2 & 2\\
0 & 0 & 3
\end{pmatrix}.
\end{align*}
Es decir, para encontrar $U$ veamos que si multiplicamos las matrices tenemos que:
\begin{align*}
2=a, \\
4=b,\\
2=2c \Rightarrow 1=c.
\end{align*}
Recordemos que si $E_1,E_2, \cdots ,E_n$ son las matrices elementales para llevar a la matriz $A$ a su forma escalonada, entonces $L=(E_n E_1 \cdots  E_1^{-1})=E_1^{-1} E_2^{-1}\cdots E_n^{-1}$. Para este problema las matrices elementales son:
\begin{align*}
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 &-1 & 1
\end{pmatrix}
\begin{pmatrix}
 1 & 0 & 0\\
 0 & 1 & 0\\
-2 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
 1 & 0 & 0\\
-3 & 1 & 0\\
 0 & 0 & 1
\end{pmatrix},
\end{align*}
por lo que 
\begin{align*}
L&= \begin{pmatrix}
 1 & 0 & 0\\
-3 & 1 & 0\\
 0 & 0 & 1
\end{pmatrix}^{-1}\begin{pmatrix}
 1 & 0 & 0\\
 0 & 1 & 0\\
-2 & 0 & 1
\end{pmatrix}^{-1}
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 &-1 & 1
\end{pmatrix}^{-1}=
\begin{pmatrix}
 1 & 0 & 0\\
 3 & 1 & 0\\
 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
 1 & 0 & 0\\
 0 & 1 & 0\\
 2 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 1 & 1
\end{pmatrix}\\ \\
&=\begin{pmatrix}
1 & 0 & 0\\
3 & 1 & 0\\
2 & 1 & 1
\end{pmatrix}
\end{align*}
Por lo tanto, la descomposición LDU de la matriz original es: 
\begin{align*}
\begin{pmatrix}
1 & 2 & 4\\
3 & 8 & 14\\
2 & 6 & 13
\end{pmatrix}=\begin{pmatrix}
1 & 0 & 0\\
3 & 1 & 0\\
2 & 1 & 1
\end{pmatrix}\begin{pmatrix}
1 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 3
\end{pmatrix}\begin{pmatrix}
1 & 2 & 4\\
0 & 1 & 1\\
0 & 0 & 1
\end{pmatrix}. \ \ \ \finf
\end{align*}
Nota, para las inversas de las matrices elementales ocupamos el siguiente teorema:
\begin{framed}
    \begin{thmt} \label{inversa_elemental}
    Sea $E_{ij}(\alpha)$ es la matriz elemental que multiplica al renglón $j$ por $\alpha$ y lo suma al renglón $i$, entonces la matriz inversa de $E_{ij}(\alpha)$ es $E_{ij}(-\alpha).$
    \end{thmt}
\end{framed} 

%Problema 2
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea 
\begin{align*}
A=\begin{pmatrix}
1 & 2 & 3\\
2 & 4 & 5\\
1 & 3 & 4
\end{pmatrix}.
\end{align*}
Observe que $A$ es no singular y siempre tiene descomposición $PLU$. Pruebe que, sin embargo, A no tiene descomposición $LU$. (Sugerencia: no use eliminación, use un teorema.)

\res 
Ocupemos el siguiente teorema (demostrado en clase)
\begin{framed}
    \begin{thmt} \label{submatrices_LU}
    Sea $A$ una matriz no singular. $A$ tiene una factorización $LU$ si y solo si todas sus submatrices principales líderes son no singulares. 
    \end{thmt}
\end{framed} 
Una submatriz principal líder se obtiene cuando es principal y $I_c=I_r=\{ 1, 2 \cdots, k\}, k<n.$\\

 Entonces si observamos la matriz principal líder $
A_{2,2}=\begin{pmatrix}
1 & 2 \\
2 & 4
\end{pmatrix}$, podemos observar que es singular debido a que el renglón 2 es múltiplo del renglón 1 (o por que su determinante es cero). Por lo tanto (ocupando el teorema \ref{submatrices_LU}), como una matriz singular principal líder es singular, esto implica que $A$ no tiene factorización $LU$. \fin \\

%Problema 3
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea 
\begin{align*}
A=\begin{pmatrix}
1 & 1 & 2 \\
1 & 3 & 8\\
2 & 8 & 23
\end{pmatrix}
\end{align*}
Encuentre la descomposición $LDL^t$ de $A$. ¿Es $A$ positiva definida? Explique. En tal caso encuentre su descomposición de Cholesky.

\res
Recordemos el siguiente lema (demostrado en clase):
\begin{framed}
    \begin{thml} \label{simetrica_LDLt}
    Sea $A$ una matriz simétrica de tamaño $n\times n$ tal que $A=LDU$ donde $L$ es triangular inferior con unos en la diagonal, $U$ es triangular superior con con unos en la diagonal, $D$ es diagonal con elementos diagonales no cero. Entonces $U=L^t$ y $A=LDL^t$.
    \end{thml}
\end{framed} 
Primero reduzcamos la matriz $A$ a su forma escalonada ocupando eliminación gaussiana:
\begin{align}\label{positiva}
\begin{pmatrix}
1 & 1 & 2 \\
1 & 3 & 8\\
2 & 8 & 23
\end{pmatrix}%
\grstep[R3 \rightarrow R_3-2R_1]{R_2 \rightarrow R_2-R_1}
%
\begin{pmatrix}
1 & 1 & 2 \\
0 & 2 & 6\\
0 & 6 & 19
\end{pmatrix}%
\grstep[]{R3 \rightarrow R_3-3R_2}
%
\begin{pmatrix}
1 & 1 & 2 \\
0 & 2 & 6\\
0 & 0 & 1
\end{pmatrix}.
\end{align}
Lo anterior implica que la matriz 
\begin{align*}
D=\begin{pmatrix}
1 & 0 & 0 \\
0 & 2 & 0\\
0 & 0 & 1
\end{pmatrix}.
\end{align*}
Las matrices elementales que se ocuparon para llegar a la forma escalonada reducida son:
\begin{align*}
\begin{pmatrix}
 1 & 0 & 0 \\
-1 & 1 & 0\\
 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0\\
-2 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0\\
0 & -3 & 1
\end{pmatrix},
\end{align*}
por lo que
\begin{align*}
L&=\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0\\
0 & -3 & 1
\end{pmatrix}^{-1}\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0\\
-2 & 0 & 1
\end{pmatrix}^{-1}\begin{pmatrix}
 1 & 0 & 0 \\
-1 & 1 & 0\\
 0 & 0 & 1
\end{pmatrix}^{-1}=
\begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0\\
0 & 3 & 1
\end{pmatrix}^{-1}\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0\\
 2 & 0 & 1
\end{pmatrix}^{-1}\begin{pmatrix}
 1 & 0 & 0 \\
 1 & 1 & 0\\
 0 & 0 & 1
\end{pmatrix}^{-1}\\ \\
&=\begin{pmatrix}
 1 & 0 & 0 \\
 1 & 1 & 0\\
 2 & 3 & 1
\end{pmatrix}.
\end{align*}
Veamos que la matriz $A$ es simétrica debido a que $A=A^t$, entonces ocupando el lema \ref{simetrica_LDLt} 
\begin{align*}
U = \begin{pmatrix}
 1 & 0 & 0 \\
 1 & 1 & 0\\
 2 & 3 & 1
\end{pmatrix}^{t} =
\begin{pmatrix}
 1 & 1 & 3 \\
 0 & 1 & 2\\
 0 & 0 & 1
\end{pmatrix} 
\end{align*}
Por lo tanto, la descomposición $LDL^t$ de $A$ es
\begin{align*}
A= \begin{pmatrix}
 1 & 0 & 0 \\
 1 & 1 & 0\\
 2 & 3 & 1
\end{pmatrix}\begin{pmatrix}
 1 & 0 & 0 \\
 0 & 2 & 0\\
 0 & 0 & 1
\end{pmatrix}\begin{pmatrix}
 1 & 1 & 3 \\
 0 & 1 & 2\\
 0 & 0 & 1
\end{pmatrix}.
\end{align*}
Como todos los pivotes de $A$ son estrictamente positivos (ver última la matriz de \ref{positiva}) entonces podemos decir que $A$ es positiva definida. \\

Una matriz diagonal $D$ con entradas positivas en la diagonal, es factorizable como $D=\sqrt{D} \sqrt{D}$, donde $\sqrt{D}$ es una matriz cuya diagonal consiste en la raiz cuadrada de cada elemento de $D$, así la descomosición de Cholesky tiene una relación con la descomposición $LDL^t$:
\begin{align*}
A=LDL^t=L(\sqrt{D} \sqrt{D})L^t)=LDL^t=(L\sqrt{D})(\sqrt{D}L^t)=(L\sqrt{D})(L\sqrt{D})^t=TT^t.
\end{align*} 
Entonces ocupando lo anterior,
\begin{align*}
\sqrt{D}= \begin{pmatrix}
 1 & 0 & 0 \\
 0 & \sqrt{2} & 0\\
 0 & 0 & 1
\end{pmatrix}\Rightarrow L\sqrt{D}= \begin{pmatrix}
 1 & 0 & 0 \\
 1 & 1 & 0\\
 2 & 3 & 1
\end{pmatrix}\begin{pmatrix}
 1 & 0 & 0 \\
 0 & \sqrt{2} & 0\\
 0 & 0 & 1
\end{pmatrix}=\begin{pmatrix}
 1 & 0 & 0 \\
 1 & \sqrt{2} & 0\\
 2 & 3\sqrt{2} & 1
\end{pmatrix}.
\end{align*} 
Por lo tanto la descomposición de Cholesky de $A$ es
\begin{align*}
A=\begin{pmatrix}
 1 & 0 & 0 \\
 1 & \sqrt{2} & 0\\
 2 & 3\sqrt{2} & 1
\end{pmatrix}\begin{pmatrix}
 1 & 1 & 2 \\
 0 & \sqrt{2} & 3\sqrt{2}\\
 0 & 0 & 1
\end{pmatrix}. \ \ \ \finf
\end{align*} 
%Problema 4
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea $A$ la matriz por bloques
\begin{align*}
A=\begin{pmatrix}
B & C \\
0 & E
\end{pmatrix}
\end{align*}
con $B$ y $E$ no singulares. Demuestre que $A^{-1}$ es de la forma
\begin{align*}
\begin{pmatrix}
B^{-1}& X \\
0 & E^{-1}
\end{pmatrix}
\end{align*}
y encuentre $X$. Luego, si 
\begin{align*}
A_1=\begin{pmatrix}
B & 0 \\
D & E
\end{pmatrix}
\end{align*}
y encuentre $Y$.

\res
Como ya demostramos que $A^{-1}$ es de la forma \begin{align*}
\begin{pmatrix}
B^{-1}& X \\
0 & E^{-1}
\end{pmatrix},
\end{align*}
entonces se tiene que cumplir que:
\begin{align*}
AA^{-1}:=\begin{pmatrix}
B & C \\
0 &E
\end{pmatrix}\begin{pmatrix}
B^{-1}& X \\
0 & E^{-1}
\end{pmatrix}=\begin{pmatrix}
I & 0 \\
0 & I
\end{pmatrix}\\
\end{align*}
%Problema 5
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea $F$ un matriz fija de $3\times 2$ y sea $$H=\{A\in M_{2\times 4}(\mathbb{R})|FA=0\}.$$
Determine si $H$ es un subespacio de $M_{2\times 4}(\mathbb{R})$.

\res
Recordemos la definición de subespacio.
\begin{framed}
    \begin{thmd} \label{subespacio}
    (Definición vista en clase) Sea $V$ un espacio vectorial sobre $K$. $W\subset V$, $W\neq 0.$ $W$ es un subespacio de $V$ si 
    \begin{enumerate}
    \item Si $w_1,w_2 \in W$ entonces $w_1+w_2\in W$.
    \item Si $w\in W,\ \alpha\in K$ entonces $\alpha w\in W.$
    \end{enumerate}
    \end{thmd}
\end{framed} 

%Problema 6
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Demuestre que en $\mR^2$ los únicos subespacioes posibles son $\{0\}$, las líneas que pasan por el origen y $\mR^2$. Enuncie y demuestre un resultado análogo para $\mR^3$.

%Problema 7
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea $S\subset \mR^3$ dado por 
\begin{align*}
S=\left\{\begin{pmatrix}
2\\
-1\\
1
\end{pmatrix}, \begin{pmatrix}
2\\
-3\\
2
\end{pmatrix} \right\}.
\end{align*}
Determine si \begin{align*}
\begin{pmatrix}
-2\\
-3\\
1
\end{pmatrix}
\end{align*}
esta en $\text{gen}(S)$, y si \begin{align*}
\begin{pmatrix}
-8\\
5\\
4
\end{pmatrix}
\end{align*}
esta en $\text{gen}(S)$.

\res 
Para que el vector $\begin{pmatrix}
-2\\
-3\\
1
\end{pmatrix}$
 este en $\gen (S)$ se debe encontrar una combinación lineal de los vectores 
$\begin{pmatrix}
2\\
-1\\
1
\end{pmatrix}$ y $\begin{pmatrix}
2\\
-3\\
2
\end{pmatrix}$ tal que sea el vector $\begin{pmatrix}
-2\\
-3\\
1
\end{pmatrix}$, es decir, sea $\alpha, \beta$ escalares 
\begin{align}\label{lineal}
\alpha \begin{pmatrix}
2\\
-1\\
1
\end{pmatrix}+\beta \begin{pmatrix}
2\\
-3\\
2
\end{pmatrix}&=\begin{pmatrix}
-2\\
-3\\
1
\end{pmatrix}.
\end{align}
Encontremos los escalares $\alpha, \beta$ que cumple (\ref{lineal}).
\begin{align*}
\begin{pmatrix}
2\alpha+2\beta\\
-\alpha-3\beta\\
\alpha+2\beta
\end{pmatrix}&=\begin{pmatrix}
-2\\
-3\\
1
\end{pmatrix}\Rightarrow\text{sumando 2 y 3}\ \ 
\beta=2 \Rightarrow \left\{\begin{matrix}
2\alpha+4=-2\\
-\alpha-6=-3\\
\alpha+4=1
\end{matrix}\right.\Rightarrow\alpha=-3.
\end{align*}
Por lo tanto, como encontramos una combinación lineal de los vectores del conjunto $S$ podemos concluir que $\begin{pmatrix}
-2\\
-3\\
1
\end{pmatrix}$ si esta en $\gen (S).$\\

Realizamos un razonamiento análogo al anterior para ver si el vector $\begin{pmatrix}
-8\\
5\\
4
\end{pmatrix}$ esta en $\gen (S)$, busquemos los vectores $\alpha$ y $\beta$ tal que 
\begin{align}\label{lineal_2}
\alpha \begin{pmatrix}
2\\
-1\\
1
\end{pmatrix}+\beta \begin{pmatrix}
2\\
-3\\
2
\end{pmatrix}&=\begin{pmatrix}
-8\\
5\\
4
\end{pmatrix}
\end{align}
Para ello,
\begin{align*}
\begin{pmatrix}
2\alpha+2\beta\\
-\alpha-3\beta\\
\alpha+2\beta
\end{pmatrix}&=\begin{pmatrix}
-8\\
5\\
4
\end{pmatrix}\Rightarrow\text{sumando 2 y 3}\ \ 
\beta=-9 \Rightarrow \left\{\begin{matrix}
2\alpha-18=-8\\
-\alpha+27=5\\
\alpha-18=4
\end{matrix}\right.\Rightarrow
\begin{array}{cc}
\alpha_1=5\\
\alpha_2=22
\end{array}\alpha_1=\alpha_2.
\end{align*}
Como no pudimos encontrar $\alpha$ y $\beta$ tal que se cumpliera (\ref{lineal_2}), es decir, no existe una combinación lineal de los vectores del conjunto $S$ que sea el vector $\begin{pmatrix}
-8\\
5\\
4
\end{pmatrix}$,  podemos concluir que $\begin{pmatrix}
-8\\
5\\
4
\end{pmatrix}$ no esta en $\gen (S).$ \ \ \ \ \fin

%Problema 8
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea $V$ un espacio vectorial y $W, \ Z$ subespacios de $V$. Al definir el espacio $W+Z$ no se hizo distinción en el orden. ¿Por qué no?, es decir, ¿es cierto que
$W + Z = Z + W$? Argumente su respuesta. Enuncie y demuestre un resultado similar para cualquier número finito de subespacios.

\res 
Recordemos la definición de la suma de dos subespacios.
\begin{framed}
    \begin{thmd} \label{suma_subespacios}
    (Definición vista en clase) Sea $V$ un espacio vectorial, $W_1,W_2$ subespacios de $V$. La suma de $W_1$ y $W_2$ es:
    \begin{align*}
    W_1+W_2=\left\{v\in V|v=w_1+w_2, w_1\in W_1, w_2\in W_2 \right\}.
    \end{align*}
    \end{thmd}
\end{framed} 
Como $W, Z$ son subespacios de $V$ y ocupando la definición \ref{subespacio}, podemos decir $\forall w\in W\Rightarrow w\in V$ y $\forall z\in Z\Rightarrow z\in V$ debido a que una condición para ser subespacio es que sea un subconjunto. Ahora ocupando la definición \ref{suma_subespacios}, tenemos que la suma 



%Problema 9
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Encuentre $A$ tal que $W=\mathcal{C}(A)$, donde
\begin{align*}
W=\left\{\left.\begin{pmatrix}
r-s\\
2r+3t\\
r+3s-3t\\
s+t
\end{pmatrix}\right| r,s,t\in \mR \right\}.
\end{align*}

\res
Recordemos la definición de espacio columna.
\begin{framed}
    \begin{thmd} \label{espacio_columna}
    El \textbf{espacio columna} de una matriz $A$ de $m\times n$, que se denota como $\mathcal{C}(A)$, es el conjunto de todas las combinacioens lineales de las columnas de $A$. Si $A=\begin{pmatrix}
    a_1 & a_2 &\cdots &a_n
    \end{pmatrix}$, entonces
    $$\mathcal{C}(A)=\gen (a_1, a_2, \cdots , a_n).$$
    \end{thmd}
\end{framed} 
Entonces para encontrar $A$, en primer lugar, escribimos $W$ como un conjunto de combinaciones lineales.
\begin{align*}
W=\left\{r\begin{pmatrix}
1\\
2\\
1\\
0
\end{pmatrix}+s\begin{pmatrix}
-1\\
0\\
3\\
1
\end{pmatrix}+\left. t\begin{pmatrix}
0\\
3\\
-3\\
1
\end{pmatrix}\right| r,s,t\in \mR\right\}=\gen\left\{\begin{pmatrix}
1\\
2\\
1\\
0
\end{pmatrix},\begin{pmatrix}
-1\\
0\\
3\\
1
\end{pmatrix}, \begin{pmatrix}
0\\
3\\
-3\\
1
\end{pmatrix}\right\}.
\end{align*}
En segundo lugar, utilizamos los vectores en el conjunto generador como las columnas de $A$. Sea $A=\begin{pmatrix}
1 &-1 & 0\\
2 & 0 & 3\\
1 & 3 &-3\\
0 & 1 & 1
\end{pmatrix}$. De esta forma, $W=\mathcal{C}(A)$.\ \ \fin
%Problema 10
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
\item Sea \begin{align*}
A=\begin{pmatrix}
 1 & -1 & 6 & 0\\
10 & -8 &-2 &-2\\
 0 &  2 & 2 &-2\\
 1 &  1 & 0 &-2
\end{pmatrix}
\end{align*}.
Encuentre un vector en $\mathcal{N}$ (A). Encuentre dos vectores distintos (que no sean múltiplos) en $\mathcal{C}(A)$. ¿Se pueden encontrar más vectores en $\mathcal{N}(A)$ y $\mathcal{C}(A)$, respectivamente, a los ya encontrados que no sean combinación lineal de los anteriores?





\end{enumerate}
\end{document}