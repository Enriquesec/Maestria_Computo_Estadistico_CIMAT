---
fontsize: 11pt
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
documentclass: article
output:
    pdf_document:
        includes:
            in_header: mystyles.sty
---
\begin{table}[ht]
\centering
\begin{tabular}{c}
\textbf{Maestría en Computo Estadístico}\\
\textbf{Inferencia Estadística} \\
\textbf{Tarea 5}\\
\today \\
\emph{Enrique Santibáñez Cortés}\\
Repositorio de Git: \href{https://github.com/Enriquesec/Inferencia_Estad-stica/tree/master/Tareas/Tarea_5}{Tarea 5, IE}.
\end{tabular}
\end{table}
Escriba de manera concisa y clara sus resultados, justificando los pasos necesarios. Serán descontados puntos de los ejercicios mal escritos y que contenga ecuaciones sin una estructura gramatical adecuada. Las conclusiones deben escribirse en el contexto del problema. Todos los programas y
simulaciones tienen que realizarse en R.

1. Supongamos que $X$ y $Y$ son v.a continuas con densidad conjunta $f$ y que $Z=Y/X$.

a) Muestre que 
\begin{align*}
F_Z(z)=\int_{-\infty}^z\intim |x| f(x, xu) dx du.
\end{align*}

\res Ocupando un cambio de variable, si proponemos la transformación
\begin{align*}
z=\frac{x}{y}\ \ \ \ \text{y si } \ \ \ \ u=x,
\end{align*}

entonces la transformación inversa es 
\begin{align*}
y=uz\ \ \ \ \text{y si } \ \ \ \ x=u,
\end{align*}

En este problema estamos suponiendo que 
$$\chi=\{(x,y)|x\in \mR, \ y\in \mR\}\Rightarrow \mathcal{Y}=\{(z,u)|z\in \mR, \ u\in \mR\},$$
esta consideración no se debe hacer tan explicita, ya que en el problema no definieron donde estan definidos $X$ y $Y$. Entonces,
\begin{align*}
J=\left| \begin{array}{cc}
u & z\\
0 & 1
\end{array} \right| \Rightarrow |J|=|u|=|x|.
\end{align*}

Así, 
\begin{align}\label{marginalerj1}
f_{Z,U}(z,u)=f(u,uz)|x| \ \ \ \ x,z\in \mR.
\end{align}

Esto implica que la marginal es
\begin{align*}
f_{Z}(z)=\intim f(u,uz)|x|du=\intim f(x,xz)|x|dx
\end{align*}

Y por lo tanto, la función acumulada es
\begin{align*}
F_Z(z)=\int_{-\infty}^z \intim f(x,xv)|x|dxdv. \ \ \ \finf
\end{align*}

b) Muestre que, si $X$ y $Y$ son independientes, la densidad de $Z$ puede escribirse como
\begin{align*}
f_Z(z)=\intim |x| \fx f_Y(xz) dx.
\end{align*}
\res Como sabemos que $X$ y $Y$ son independientes podemos simplificar el resultado obtenido en (\ref{marginalerj1}) para encontrar la densidad de $Z$.
\begin{align*}
f_{Z}(z)=\intim f(u,uz)|x|du=\intim f(x,xz)|x|dx=\intim \fx f_Y(xz)|x|dx. \ \ \ \finf
\end{align*}

c) Si $X$ y $Y$ son independientes con distribución normal estándar, muestre que $Z$ se distribuye como una Cauchy. ¿Qué parametrización tiene la densidad de esta distribución?

\res Como sabemos que $X,Y$ son independientes con distribución normal estándar, entonces las funciones marginales son
\begin{align*}
\fx =\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \ \ \ \ \text{y}\ \ \fy =\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}
\end{align*}
Entonces ocupando el resultado en el incido $b)$ tenemos que la densidad de $Z$ es
\begin{align*}
f_{Z}(z)&=\intim \fx f_Y(xz)|x|dx=\intim \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\frac{1}{\sqrt{2\pi}}e^{-\frac{(xz)^2}{2}}|x|dx=\frac{1}{2\pi}\intim e^{-x^2(z^2+1)/2}|x|dx\\
&=\frac{1}{2\pi}\left[ -\int_{-\infty}^0 e^{-x^2(z^2+1)/2}xdx+\int_{0}^\infty e^{-x^2(z^2+1)/2}xdx \right].
\end{align*}
Haciendo un cambio de variable $m=-x^2(z^2+1)/2\Rightarrow dm=-x(z^2+1)$ tenemos que 
\begin{align*}
f_{Z}(z)&=\frac{1}{2\pi}\frac{1}{(z^2+1)}\left[ \int_{-\infty}^0 e^mdm-\int_{0}^\infty e^{m}dm \right]=\frac{1}{2\pi(z^2+1)}\left[ \left.e^{-x^2(z^2+1)/2}\right|_{x\rightarrow-\infty}^0-\left.e^{-x^2(z^2+1)/2}\right|_{x=0}^{x\rightarrow\infty} \right]\\
&=\frac{1}{2\pi(z^2+1)}\left[ 1+1 \right]=\frac{1}{\pi(z^2+1)}. \ \ \-\infty <z < \infty.
\end{align*}
Por lo tanto, \textbf{$Z$ se distribuye como una Cauchy(0,1)}.\ \ \fin

2. Sea $Y$ una v.a discreta que toma valores de $\{ 0, 1, 2, \cdots \}$ y $X|Y=y\sim$ Binomial$(n=y,p)$. Muestre lo siguiente.

a) Si $Y$ tiene distribución de Poisson con media $\theta$, entonces la distribución marginal de $X$ es Poisson con media $p\theta$.

\res Tenemos que la función de densidad condicional de $X$ dado $Y=y$ es
\begin{align*}
f_{X|y}(x)={y \choose x}p^{x}(1-p)^{y-x}, \ \ \  0\leq x \leq y,
\end{align*}
y la marginal de $Y$ es
\begin{align*}
\fy=\frac{\theta^ye^{-\theta}}{y!}, y=0,1,2,\cdots
\end{align*}
entonces la marginal de $X$ esta dada por 
\begin{align*}
\fx&=\sum_{y=0}^\infty f_{X|y}(x)\fy=\sum_{y=0}^\infty {y \choose x}p^{x}(1-p)^{y-x} \frac{\theta^ye^{-\theta}}{y!}\\
&=\sum_{y=0}^\infty \frac{y!}{x!(y-x)!}p^{x}(1-p)^{y-x} \frac{\theta^ye^{-\theta}}{y!}=\frac{p^xe^{-\theta}}{x!}\sum_{y=0}^\infty \frac{(1-p)^{y-x}\theta^y}{(y-x)!}
\end{align*}
ahora realizamos un cambio de variable, $m=(y-x)$(o $y=m+x$) esto implica que 
\begin{align*}
\fx =\frac{p^xe^{-\theta}}{x!}\sum_{y=0}^\infty \frac{(1-p)^{y-x}\theta^y}{(y-x)!}=\frac{p^xe^{-\theta}}{x!}\sum_{m=0}^\infty \frac{(1-p)^m\theta^{m+x}}{m!}= \frac{p^x\theta^x e^{-\theta}}{x!}\sum_{m=0}^\infty \frac{[(1-p)\theta]^m}{m!}
\end{align*}
Ahora ocupando la definición de exponencial (\ref{exponencial}), podemos concluir que 
\begin{align*}
\fx =\frac{p^x\theta^x e^{-\theta}}{x!}\sum_{m=0}^\infty \frac{[(1-p)\theta]^m}{m!} = \frac{(p\theta)^x e^{-\theta}}{x!} e^{(1-p)\theta}=\frac{(p\theta)^x e^{-p\theta}}{x!}.
\end{align*}
Por lo tanto, \textbf{podemos concluir que $X$ se distribuye como una Poisson con media $p\theta$}. 
\begin{framed}
    \begin{thmd} \label{exponencial}
	La función exponencial se puede expresar como la siguiente serie
	\begin{align*}
	e^x=\sum_{n=0}^\infty \frac{x^n}{n!}.
	\end{align*}
    \end{thmd}
\end{framed}

b) Si $Y+r$ tiene distribución Binomial Negativa con tamaño $r$ y probabilidad $\pi$, la distribución marginal de $X+r$ es la distribución Binomial Negativa con tamaño $r$ y probabilidad $\pi/(1-(1-p)(1-\pi))$.
Hint: Utiliza funciones generadoras de momentos.

\res Tenemos que si 
$$M_{X|Y}(t)=a_1(t)e^{a_2(t)Y}$$
donde $a_i(t)$ son funcioens de $t$, entonces podemos calcular la función generadora de momentos de $X$ como
\begin{align}\label{generadora}
M_X(t)=\mE[M_{X|Y}(t)]=a_1(t)M_Y(a_2(t)).
\end{align} 
Entonces como $X+r|Y+r=y\sim \text{Binomial}(n=y,p)$, entonces recordando la generatriz de momentos de una v.a. Binomial tenemos que
\begin{align*}
M_{X+r|Y+r}(t)=[(1-p)+pe^t]^{Y+r}=e^{\ln(1-p+pe^{t})(Y+r)}=a_1(t)e^{a_2(t)(Y+r)}.
\end{align*}
donde $a_1(t)=1, \ a_2(t)=\ln(1-p+pe^{t})$. Entonces ocupando la propiedad de la generadora de momentos (\ref{generadora}) tenemos que la generadora de momentos marginal de $X$ es
\begin{align*}
M_{X+r}(t)&=M_{Y+r}(\ln(1-p+pe^{t}))=\left(\frac{\pi}{1-(1-\pi)e^{\ln(1-p+p e^t)}} \right)^r =\left(\frac{\pi}{1-(1-\pi)(1-p+p e^t)} \right)^r \\
&=\left(\frac{\pi}{1-(1-\pi)(1-p)-(1-\pi)pe^t)} \right)^r=
\left(\frac{\pi}{\pi+p-p\pi-(1-\pi)pe^t)} \right)^r\\
&=\left(\frac{\frac{\pi}{\pi+p-p\pi}}{1-\frac{(1-\pi)pe^t)}{\pi+p-p\pi}} \right)^r=\left(\frac{\frac{\pi}{\pi+p-p\pi}}{1-\frac{p-\pi p}{\pi+p-p\pi}e^t }\right)^r=\left(\frac{\frac{\pi}{\pi+p-p\pi}}{1-\frac{\pi+p-\pi p-\pi}{\pi+p-p\pi}e^t }\right)^r\\
&=\left(\frac{\frac{\pi}{\pi+p-p\pi}}{1-\left(1-\frac{\pi}{\pi+p-p\pi}\right)e^t }\right)^r=\left(\frac{\frac{\pi}{\pi+p-p\pi}}{1-\left(1-\frac{\pi}{\pi+p-p\pi}\right)e^t }\right)^r
\end{align*}
Observemos que la generadora de momentos obtenida es de una Binomial con parametros $r$ y probabilidad  $\frac{\pi}{\pi+p-p\pi}$, es decir, \textbf{podemos concluir que $X+r\sim BinomialNegativa(r, \frac{\pi}{\pi+p-p\pi})$.} \ \ \ \ \fin

3. Sea $N$ una v.a. con esperanza finita y $X_1, X_2, \cdots$ v.a. independientes de $N$ con media común $\mE(X).$ Definamos $T=\sum_{i=1}^N X_i$. Muestre que 
$$\mE(T)=\mE(N\cdot \mE(X))=\mE(N)\mE(X).$$
Si además asumimos que las $X_i's$ tiene varianza común $Var(X)$, también demuestre que 
\begin{align*}
Var(T)=\mE(N)Var(X)+\mE(X)^2Var(N).
\end{align*}

\res Para calcular la esperanza de $T$ recordemos la ley de probabilidad total
\begin{align*}
\mE[g(X)]=\mE[\mE[g(X)|Y]].
\end{align*}
Entonces, ocupando esta propiedad y las propiedades básicas de esperanza tenemos que 
\begin{align*}
\mE[T]=\mE[\mE[T|N]]=\mE[\mE[\sum_{i=1}^N X_i|N]]=\mE[N\mE[X]]=\mE[N]\mE[X],
\end{align*}
es decir, \textbf{queda demostrado que $\mE(T)=\mE(N\cdot \mE(X))=\mE(N)\mE(X).$} Ahora calculemos la varianza de $T$, para ello ocupemos la definición de la varianza, después agregamos un cero ($N\mE[X]-N\mE[X]$), separamos el cuadrado, 
ocupamos la ley de probabilidad total en todos los términos, posteriormente observamos que los primeros dos términos son las varianzas de las mismas variables y el último termino se anula.
\begin{align*}
Var[T]&=\mE[(T-\mE[N]\mE[X])^2]\\
&=\mE[(T-N\mE[X]+N\mE[X]-\mE[N]\mE[X])^2]\\
&=\mE[(T-N\mE[X])^2]+\mE[\mE[X]^2(N-\mE[N])^2]+2\mE[\mE[X](T-N\mE[X])(N-\mE[N])]\\
&=\mE[\mE[(T-N\mE[X])^2|N]]+\mE[X]^2\mE[(N-\mE[N])^2]+2\mE[X]\mE[\mE[(T-N\mE[X])(N-\mE[N])|N]]\\
&=\mE[\mE[\left(\sum_i^nX_i-N\mE[X]\right)^2|N]]+\mE[X]^2Var(N)+2\mE[X]\mE[\mE[(T-N\mE[X])|N]\mE[N-\mE[N])|N]]\\
&=\mE[\mE[\left(\sum_i^nX_i-N\mE[X]\right)^2|N]]+\mE[X]^2Var(N)+2\mE[X]\mE[\mE[(T-N\mE[X])|N](\mE[N]-\mE[N])]\\
&=\mE[NVar(X)]+\mE[X]^2Var(N)=\mE[N]Var(X)]+\mE[X]^2Var(N),
\end{align*}
es decir, \textbf{queda demostrado que $Var(T)=\mE[N]Var(X)]+\mE[X]^2Var(N).$} \ \ \ \fin

4. En este ejercicio corroborará mediante simulaciones la Ley de los Grandes Números (LGN).

a) Simule una muestra $\{x_1, \cdots, x_n\}$ de una v.a. Normal$(\pi, \sqrt{2})$ con tamaño $n=10^5$. Defina $y_m=\sum_{i=1}^m x_i/m$ y grafique esta cantidad como función de $m$. ¿Qué observa? ¿Cómo está esto relacionado con la LGN?

\res Creemos la muestra de una Normal$(\pi, \sqrt{2})$, y para cada $m$ calculemos $\bar{X}$. Para graficar los puntos ocupamos la librería $scattermore$ la cuál ayuda a graficar más rapido muchos puntos en ggplot. 
```{r, message=FALSE, warning=FALSE}
library(tidyverse) # ggplot, dplyr
library(scattermore) # graficar muchos puntos 
library(latex2exp) # legendas de las gráficas
set.seed(08081997)
muestra_x_n <- rnorm(10**5, mean=pi, sd=sqrt(2))
y_m <- data.frame(y=cumsum(muestra_x_n)/seq(1,10**5), m=seq(1,10**5))
ggplot(y_m, aes(x=m, y=y))+
  geom_scattermore()+
  labs(title=unname(TeX("Simulaciones de $\\bar{X}$, de una v.a  Normal($\\pi, \\sqrt{2}) ")))
```
Observamos que cuando $n$ se acerca a $10^5$, $\bar{X}$ se parece más a $\pi$. Si recordamos la Ley de los Grandes Números:
\begin{framed}
    \begin{thmt} \label{lgn}
	Si $X_1,\cdots, X_n$ son v.a.i.i.d entonces $$\bar{X}_n\rightarrow^P \mu.$$
	Es decir, $\bar{X}_n$ converge en probabilidad a $\mE(X)$ cuando $n\rightarrow\infty$.
    \end{thmt}
\end{framed}
Entonces, \textbf{se comprueba que la ley de grandes números se cumple considerando que las $X_i$ son v.a.i.i.d con distribución Normal.}.

b) Repita el procedimiento anterior 100 veces y grafique las $y_m$ de cada iteración sobre una misma gráfica. ¿Qué observa?

\res Cremos una función para repetir la función anterior $m$ veces.
```{r, message=FALSE, warning=FALSE }
lgn_x_n <- function(n){
  muestras_x_n <- rnorm(10**5*n, mean=pi, sd=sqrt(2)) # generamos 10^5n*m muestras.
  y_m <- c() # inicializamos
  m_n <- c()
  for(i in 0:(n-1)){ # iteramos las n veces
    m_n <- c(m_n, seq(1,10**5)) 
    recorte <- muestras_x_n[(i*10**5+1):((i+1)*10**5)]
    y_m <- c(y_m, cumsum(recorte)/seq(1,10**5)) # calculamos x_bar.
  }
  y_m <- data.frame(y=y_m, m=m_n) # m muestras normales
}
```

Utilizando esta función generamos 100 veces el procedimiento del inciso a) y gráficamos.
```{r  message=FALSE, warning=FALSE,}
set.seed(08081997) # semilla
y_m_100 <- lgn_x_n(100) # repetimos 100 veces el procedimiento

ggplot(y_m_100, aes(x=m, y=y))+ # gráficamos
  geom_scattermore()+
  labs(title=unname(TeX("100 Simulaciones de $\\bar{X}$, de una v.a  Normal($\\pi, \\sqrt{2}) ")))
```

Observamos claramente que las 100 repeticiones convergen a $\pi$. Es decir, para cada repetición se cumple la ley de grandes números.

c) Repita los dos incisos anteriores para una distribución Cauchy$(\pi, 2)$. ¿Qué observa?

\res Antes de realizar lo solicitado recodemos que la media de la distribución Cauchy no esta definida, por lo que se espera la ley de grandes números no sea clara.

```{r  message=FALSE, warning=FALSE}
set.seed(08081997)
muestra_x_n <- rcauchy(10**5, pi, 2)
y_m <- data.frame(y=cumsum(muestra_x_n)/seq(1,10**5), m=seq(1,10**5))
ggplot(y_m, aes(x=m, y=y))+
  geom_scattermore()+
  labs(title=unname(TeX("Simulaciones de $\\bar{X}$, de una v.a  Cauchy($\\pi, 2) ")))
```

Observamos claramente que no existe convergencia. Ahora generamos nuevemente una función pero ahora considerando que se usará una distribución Cauchy.

```{r  message=FALSE, warning=FALSE, fig.width = 5.2, fig.asp = 0.62, fig.align = "center"}
lgn_cauchy_x_n <- function(n){
  muestras_x_n <- rcauchy(10**5*n, pi, 2)
  y_m <- c()
  m_n <- c()
  for(i in 0:(n-1)){
    m_n <- c(m_n, seq(1,10**5))
    recorte <- muestras_x_n[(i*10**5+1):((i+1)*10**5)]
    y_m <- c(y_m, cumsum(recorte)/seq(1,10**5))
  }
  y_m <- data.frame(y=y_m, m=m_n)
}
```
Aplicamos la función anterior y gráficamos.
```{r}
set.seed(08081997)
y_m_100 <- lgn_cauchy_x_n(100)

ggplot(y_m_100, aes(x=m, y=y))+
  geom_scattermore()+
  labs(title=unname(TeX("100 Simulaciones de $\\bar{X}$, de una v.a  Cauchy($\\pi, 2) ")))
```

En efecto como se suposo al principio, no esta clara la convergencia en este caso, es decir, como la media de una distribución Cauchy no esta definida esto implica que la ley de grandes números no este definido. \ \ \ \fin

5. El número de carros que pasa un cruce durante una hora tiene distribución de Poisson de parámetro $\lambda$. El número de personas en cada carro tiene distribución de Poisson de parámetro $\mu$. Si $Y$ es el total de personas que pasan por el cruce durante una hora, calcule $\mE(Y)$ y $Var(Y)$.

\res Denotemos a $X$ una v.a. como el número de carros que pasan un cruce durante una hora, tal que $X\sim Poisson(\lambda)$ y a $W$ el número de personas de cada carro tal que $W\sim Poisson(\mu)$. Nosotros estamos suponiendo que $X$ y $W$ son independientes debido a que consideramos que la cantidad de personas en un carro no depende de la frecuencia de los carros que cruzan el semáforo. Entonces el número total de personas que pasan por el cruce durante una hora es $Y=XW$. Con lo anterior podemos calcular la esperanza y varianza de $Y$. 
\begin{align*}
\mE[Y]=\mE[XW]=\mE[X]\mE[Y]=\lambda \mu.
\end{align*}
Ahora la varianza,
\begin{align*}
Var[Y]&=Var[XW]=\mE[(XW)^2]-(\mE[XW])^2=\mE[X^2]\mE[W^2]-\mE[X]^2\mE[W]^2\\
&=(\lambda+\lambda^2)(\mu+\mu^2)-\lambda^2\mu^2 =\lambda^2\mu+\lambda\mu+\lambda\mu^2=\mu \lambda(\lambda+1+\mu).
\end{align*}
Por lo tanto, la esperanza del número total que pasan por el cruce durante una hora esta dado por el número total esperado de carros por el número esperado de personas en cada carro. Y la varianza igual tiene relación con las varianzas de $X$ y $W$. \ \ \ \ \fin

6. Sea $p$ la probabilidad de que un chinche caiga con la punta hacia abajo al lanzarlo una vez. Una persona lanza un chinche hasta que la punta caiga hacia abajo por primera vez. Sea $X$ el número de lanzamientos. Luego lanza de nuevo el chinche otras $X$ veces. Sea $Y$ el número de veces que la punta del chinche cae hacia abajo en la segunda serie de lanzamientos. Halle la distribución de $Y$.

\res Por como esta definido $X$ podemos decir, que $X\sim Geo(p)$, por lo que la distribución marginal de $X$ es
\begin{align*}
\fx = (1-p)^{x-1}p, \ \ \ \ \ x=1,2,\cdots
\end{align*}
Además recordemos que una v.a. con distribución Geométrica tiene una relación de igualdad con una v.a. con distribución Binomial Negativa,
\begin{align*}
X\sim Geometica(p)\Leftrightarrow X\sim BinomialNegativa(r=1,p).
\end{align*}

Y como $Y$ es el número de veces que la punta del chinche cae hacia abajo en la segunda ronda, podemos decir que  $Y|X=x\sim$ Binomial $(n=x,p)$. Entonces tomando esas consideraciones podemos determinar la densidad de $Y$, para ello recordemos que el ejercicio 2 b) se demostró que si $Y|X=x\sim$ Binomial $(n=x, p)$ y $X\sim$ Binomial Negativa $( r,\pi)$ esto implica que $Y\sim$ Binomial Negativa $\left(r, \frac{\pi}{\pi+p-p\pi} \right)$.

Entonces como tenemos que $Y|X=x\sim Binomial(n=x,p)$ y $X\sim$  BinomialNegativa$(r=1,p)$ podemos ocupar el resultado del ejercicio 2 y concluir que $Y$ \textbf{ el número de veces que la punta de la chinche cae hacia abajo en la segunda serie de lanzamientos se distribuye como una distribución Binomial Negativa}$(r=1,\frac{1}{2-p} )$, ya que $\pi=p$ en este problema. Y por lo tanto, la distribución de $Y$ es
\begin{align*}
\fx = \left(1-\frac{1}{2-p}\right)^{x-1}\frac{1}{2-p}, \ \ \ \ \ x=1,2,\cdots\ \ \ \ \ \ \finf
\end{align*}

7. Sean $(X_1, \cdots, X_n)$ v.a.i.i.d. con $X_i\sim N(0,1)$. Muestre que $X_i^2/\sum_{j=1}^nX_j^2$ y $\sum_{j=1}^nX_{j}^2$ son independientes, $i=1,2,\cdots$.

\res Primero demostremos que las $X_i^2$ son independientes, entonces como las $X_i$ tienen distribución norma y además son independientes podemos decir que la distribución conjunta es
\begin{align*}
f_{X_1, \cdots, X_n}(x_1, \cdots x_2)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x_1^2}\cdots\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x_n^2}=\left(\frac{1}{\sqrt{2\pi}}\right)^n e^{-\frac{1}{2}\sum_{i=1}^nx_i^2}, \ \ \ \forall i\ -\infty< x_i<\infty.
\end{align*}
Entonces, consideremos las transformaciones $z_i=x_i^2,\ \forall i,$ veamos que estas transformaciones no son ya que $(-x_i)^2=x_i^2$. Entonces para tener una transformación uno a uno podemos partir 
$$\chi_i=\left\{x_i| x_i\in (-\infty,\infty), \right\}$$ como
$$\chi_{i_1}=\left\{x_i| x_i\in (0,\infty)  \right\}, \ \ \ \chi_{i_2}=\left\{x_i| x_i\in (-\infty,0) \right\}$$
Considerando estas nuevas áreas podemos definir transformaciones uno a uno para las $X_i$, $z_{i1}=x_{i1}^2 \ \ x_{i1}\in \mR^+$ y $z_{i2}=x_{i2}^2 \ \ x_{i2}\in \mR^-$. Entonces tenemos que $x_{i1}=\sqrt{z_{i1}} \ \ x_{i1}\in \mR^+$ y $x_{i2}^2=\sqrt{z_{i2}} \ \ x_{i2}\in \mR^-$ pero como $x_{i1}+x_{i2}=x_i$ y además $z_{i1}=z_{i2}$ entonces podemos  hacer la transformada inversa uno a uno $x_i=2\sqrt{z_i}$
y por lo tanto el Jaccobiano es 
\begin{align*}
J=\left|\begin{array}{cccc}
\frac{2}{2\sqrt{z_1}} & 0 &\cdots & 0\\
0 & \frac{2}{2\sqrt{z_2}} &\cdots & 0\\
\vdots & \vdots & \vdots & \vdots\\
0 &0& \cdots & \frac{2}{2\sqrt{z_n}} 
\end{array} \right|= \frac{1}{\sqrt{z_1z_2\cdots z_n}}, \Rightarrow |J|=\frac{1}{\sqrt{z_1z_2\cdots z_n}}.
\end{align*} 
Entonces la densidad conjunta de las $Z_i$ es 
\begin{align*}
f_{Z_1, \cdots, Z_n}(z_1, \cdots z_n)&=\left(\frac{1}{\sqrt{2\pi}}\right)^n e^{-\frac{1}{2}\sum_{i=1}^n(\sqrt{z_i})^2}\frac{1}{\sqrt{z_1z_2\cdots z_n}}\\
&=\left(\frac{1}{\sqrt{2\pi}}\right)^n e^{-\frac{1}{2}\sum_{i=1}^n z_i}\frac{1}{\sqrt{z_1z_2\cdots z_n}}, \ \ \forall i, z_i>0.
\end{align*}
Entonces la marginal de algún $Z_i$ es  (abusando un poco la notación)
\begin{align*}
f_{Z_i}(z_i)&=\int_0^\infty\cdots \int_0^\infty\left(\frac{1}{\sqrt{2\pi}}\right)^n e^{-\frac{1}{2}\sum_{i=1}^n z_i}\frac{1}{\sqrt{z_1z_2\cdots z_n}}dz_1\cdots dz_{i-1} dz_{i+1}\cdots dz_n \\
&=\left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} z_i}\frac{1}{\sqrt{z_i}}\int_0^\infty \left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} z_1}\left(\frac{1}{2}\right)\frac{1}{\sqrt{z_1}} dz_1 \cdots \int_0^\infty e^{-\frac{1}{2} z_{i-1}}\frac{1}{\sqrt{z_{i-1}}} dz_{i-1} \times \\
&\int_0^\infty \left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} z_{i+1}}\frac{1}{\sqrt{z_{i+1}}} dz_{i+1}\cdots\int_0^\infty \left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} z_{n}}\frac{1}{\sqrt{z_{n}}} dz_{n}
\end{align*}
Observando que las integrales representan a la integral de la densidad de una distribución $\chi^2$, entonces podemos simplificar cada una a 1. Por lo que la marginal de $Z_i$ es 
\begin{align*}
f_{Z_i}(z_i)=\left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} z_i}\frac{1}{\sqrt{z_i}}. \ \ \ z_i>0.
\end{align*}
Entonces como lo hicimos para cualquier $i$ entonces se cumple para cualquier marginal de $Z_i$. Entonces, con lo anterior es sencillo ver que se cumple (no se probó por que es trivial)
\begin{align*}
f_{Z_1, \cdots, Z_n}(z_1, \cdots z_2) =f_{Z_1}(z_1)\cdots f_{Z_n}(z_n).
\end{align*}
por lo que podemos concluir que las $Z_i$ son independientes. 

Tenemos entonces que $Z_1=X_1^2, Z_2=X_2^2\cdots, Z_n=X_n^2$ son independientes. Entonces definamos las siguientes transformaciones
\begin{align*}
t_1=\frac{Z_1}{\sum_{i=1}^{n} Z_i},\  t_2=\frac{Z_2}{\sum_{i=1}^{n} Z_i}, \cdots, t_{n-1}=\frac{Z_{n-1}}{\sum_{i=1}^{n} Z_i},\ \ \text{y} \ \ t_n=\sum_{i=1}^{n} Z_i.
\end{align*}
Por lo que las transformaciones inversas son 
\begin{align*}
Z_k=t_k\left(\sum_{i=1}^{n} Z_i\right)\Leftrightarrow Z_k=t_1(t_n)\ \ \ 0<k<n, \text{ y} \ \ Z_n=t_n-\sum_{i=1}^{n-1} Z_i=t_n-t_n\sum_{i=n}^{n-1}t_i.
\end{align*}
veamos que son transformaciones uno a uno. El Jacobiano es,
\begin{align*}
J=\left|\begin{array}{ccccc}
t_n& 0 & \cdots&0 & t_1\\
0 & t_n&\cdots&0 & t_2\\
\vdots & \vdots & \vdots& \vdots & \vdots\\
0 & 0 & \cdots& t_{n} &t_{n-1}\\
-t_n & -t_n &\cdots & -t_n & 1-\sum_{i=1}^{n-1} t_i
\end{array} \right| = \left|\begin{array}{ccccc}
t_n& 0 & \cdots&0 & t_1\\
0 & t_n&\cdots&0 & t_2\\
\vdots & \vdots & \vdots& \vdots & \vdots\\
0 & 0 & \cdots& t_{n} &t_{n-1}\\
0 & 0 &\cdots & 0 & 1
\end{array} \right|=t_n^{n-1}.
\end{align*}
Por lo tanto, densidad conjunta de las $T_i's$ es  
\begin{align*}
f_{T_1, \cdots, T_n}(t_1, \cdots t_n)&=f_{Z_1}\left( t_1 t_n \right)\times \cdots\times f_{Z_{n-1}}\left( t_1 t_{n-1} \right) f_{Z_n}\left(t_n-t_n\sum_{i=n}^{n-1}t_i \right)\\
&=\left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} t_1t_n}\frac{1}{\sqrt{t_1t_n}}\times \cdots\times\left(\frac{1}{\sqrt{2\pi}}\right)e^{-\frac{1}{2} t_{n-1}t_n}\frac{1}{\sqrt{t_{n-1}t_n}}
\left(\frac{1}{\sqrt{2\pi}}\right)\frac{e^{-\frac{1}{2}( t_n-t_n\sum_{i=n}^{n-1}t_i)}}{\sqrt{t_n-t_n\sum_{i=n}^{n-1}t_i}}\times t_n^{n-1}\\
&=\left(\frac{1}{\sqrt{2\pi}} \right)^n e^{-\frac{1}{2}t_n\sum_{i=1}^{n-1}t_i+\frac{1}{2}t_n\sum_{i=1}^{n-1}t_i} e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n(1-\sum_{i=1}^{n-1}t_i)}} \frac{1}{\sqrt{t_n^{n-1}\prod_{i=1}^{n-1}t_i}}t_n^{n-1}\\
&=\left(\frac{1}{\sqrt{2\pi}} \right)^n e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}}\frac{1}{\sqrt{\prod_{i=1}^{n-1}t_i(1-\sum_{i=1}^{n-1}t_i)}}\ \ \ \ \ \forall i, t_i>0
\end{align*}
Calculemos la distribución conjunta de las $T_1, \cdots, T_{n-1}$, 
\begin{align*}
f_{T_1, \cdots, T_{n-1}}(t_1, \cdots t_{n-1})&= \int_0^\infty\left(\frac{1}{\sqrt{2\pi}} \right)^n e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}}\frac{1}{\sqrt{\prod_{i=1}^{n-1}t_i(1-\sum_{i=1}^{n-1}t_i)}} dt_n\\
&=\left(\frac{1}{\sqrt{2\pi}} \right)^{n-1}\frac{1}{\sqrt{\prod_{i=1}^{n-1}t_i(1-\sum_{i=1}^{n-1}t_i)}} \int_0^\infty\left(\frac{1}{\sqrt{2\pi}} \right) e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}} dt_n\\
&= \left(\frac{1}{\sqrt{2\pi}} \right)^{n-1}\frac{1}{\sqrt{\prod_{i=1}^{n-1}t_i(1-\sum_{i=1}^{n-1}t_i)}}, \ \ \forall i, t_i>0
\end{align*} 
Ahora calculemos la marginal de $T_n$,
\begin{align*}
f_{T_n}(t_n)&= \int_0^\infty\cdots \int_0^\infty \left(\frac{1}{\sqrt{2\pi}} \right)^n e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}}\frac{1}{\sqrt{\prod_{i=1}^{n-1}t_i(1-\sum_{i=1}^{n-1}t_i)}} dt_1\cdots dt_{n-1}\\
&=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}}\int_0^\infty\cdots \int_0^\infty \left(\frac{1}{\sqrt{2\pi}} \right)^{n-1} \frac{1}{\sqrt{\prod_{i=1}^{n-1}t_i(1-\sum_{i=1}^{n-1}t_i)}} dt_1\cdots dt_{n-1}\\
&=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}}\int_0^\infty\cdots \int_0^\infty f_{T_1, \cdots, T_{n-1}}(t_1, \cdots t_{n-1})dt_1\cdots dt_{n-1}\\
&=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}t_n}\frac{1}{\sqrt{t_n}} , \ \ \forall i, t_i>0
\end{align*}
en lo anterior se ocupo la propiedad de la función de probabilidad conjunta integral a uno considerando todas las v.a. Entonces como $f_{T_1, \cdots, T_{n-1}}(t_1, \cdots t_n)=f_{T_1, \cdots, T_{n-1}}(t_1, \cdots t_{n-1})f_{T_n}(t_n)$ podemos concluir que el conjunto $T_1, \cdots, T_{n-1}$ es independiente de $T_n$. Es decir, el conjunto $X_i^2/\sum_{j=1}^nX_j^2$ para $i=1,2,\cdots , n-1$ y $\sum_{j=1}^nX_{j}^2$ son independientes.

Ahora con el mismo procedimiento podemos demostrar que cualquier conjunto de las $X_i^2/\sum_{j=1}^nX_j^2$ de $n-1$ elementos es independiente a $\sum_{j=1}^nX_{j}^2$ (la demostración se puede hacer por inducción pero se omitió por el hecho que se realiza el mismo procedimiento descrito anterior), por lo tanto podemos concluir que el conjunto $X_i^2/\sum_{j=1}^nX_j^2$ para $i=1,2,\cdots , n$ es independiente a $\sum_{j=1}^nX_{j}^2$. \ \ \ \ \ \  \fin

8. Sean $(X_1,\cdots,X_n)$ v.a.i.i.d tal que $X_i\sim Exponencial(\theta)$. Sean $X_{(1)}\leq X_{(2)}\leq \cdots \leq X_{(n)}$ estadísticos de orden, $X_{(0)}=0$ y $Z_i=X_{(i)}-X_{(i-1)},\ i=1,2,\cdots ,n.$ Calcule la distribución de $2(n-i+1)Z_i/\theta.$ ¿Son $Z_1,\cdots, Z_n$ independientes?

\res Calculemos la distribución conjunta de los estadísticos de orden $X_{(1)}, X_{(2)}, \cdots, X_{(n)}$, para ello ocupemos el siguiente teorema 
\begin{framed}
    \begin{thmt} \label{marginales}
	Sean $(X_1,\cdots,X_n)$ y $X_{(1)}\leq X_{(2)}\leq \cdots \leq X_{(n)}$ estadísticos de orden, entonces si $F_X(x)$ es continua entonces 
	\begin{align*}
	f_{X_{(1)}, X_{(2)}, \cdots, X_{(n)}}(x_1, x_2, \cdots, x_n)=n!f_X(x_1)f_X(x_2)\cdots f_X(x_n), \ \ \ \ x_1\leq x_2 \leq \cdots \leq x_n.
	\end{align*}
    \end{thmt}
\end{framed}
Entonces, podemos decir que la densidad conjunta de los estadísticos de orden para este problema es
\begin{align*}
f_{X_{(1)}, X_{(2)}, \cdots, X_{(n)}}(x_1, x_2, \cdots, x_n) = n!\frac{1}{\theta} e^{-x_1/\theta}\cdots \frac{1}{\theta} e^{-x_n/\theta} = n!\left(\frac{1}{\theta} \right)^n e^{-\frac{\sum_{i=1}^n x_i}{\theta}}
\end{align*}
Entonces considerando las transformaciones 
\begin{align*}
z_1=x_{(1)}, \ \ \ \ \ \ z_2=x_{(2)}-x_{(1)},\ \ \ \ \cdots, \ \ \ z_n=x_{(n)}-x_{(n-1)}
\end{align*}
por lo tanto las inversas de las transformaciones es
\begin{align*}
x_{(1)}=z_1, \ \ \ \ \ \ x_{(2)}=z_2+x_{(1)}=z_2+z_1,\ \ \ \cdots, \ \ \ x_{(n)}=z_n+x_{(n-1)}=z_n+z_{n-1}+\cdots+z_2+z_1.
\end{align*}
Y el Jacobiano es
\begin{align*}
J=\left|\begin{array}{ccccc}
1 & 0 & 0& \cdots & 0\\
1 & 1 & 0& \cdots & 0\\
\vdots & \vdots & \vdots & \vdots\\
1 & 1 & 1& \cdots & 1\\
\end{array} \right|=1\Rightarrow |J|=1.
\end{align*}
Por lo tanto, la distribución conjunta de las $Z_i$ es
\begin{align*}
f_{Z_1, \cdots, Z_n}(z_1, \cdots z_n)&=n!\left(\frac{1}{\theta} \right)^n e^{-\frac{z_1+\sum_{i=1}^2z_i+\cdots+\sum_{i=1}^nz_i}{\theta}}=n!\left(\frac{1}{\theta} \right)^n e^{-\frac{nz_1+(n-1)z_2+\cdots+2z_{n-1}+z_n}{\theta}}\\
&=n\left(\frac{1}{\theta} \right) e^{-\frac{nz_1}{\theta}}(n-1)\left(\frac{1}{\theta} \right) e^{-\frac{(n-1)z_2}{\theta}}\times \cdots \times\left(\frac{1}{\theta} \right) e^{-\frac{z_n}{\theta}}.
\end{align*}
Entonces la marginal de $Z_i$ es igual a integral $n-1$ veces con respecto a todas las $Z_k$ tal que $k\neq i$, es decir,
\begin{align*}
f_{Z_i}(z_i)&=\int_0^\infty\cdots\int_0^\infty n\left(\frac{1}{\theta} \right) e^{-\frac{nz_1}{\theta}} \times \cdots \times \left(\frac{1}{\theta} \right) e^{-\frac{z_n}{\theta}}dz_1\cdots dz_{i-1}dz_{i+1} \cdots dz_{n}\\
&=(n-i+1)\left(\frac{1}{\theta} \right) e^{-\frac{(n-i+1)z_{i}}{\theta}} \int_0^\infty\cdots\int_0^\infty n\left(\frac{1}{\theta} \right) e^{-\frac{nz_1}{\theta}} \times \cdots \times \left(\frac{1}{\theta} \right) e^{-\frac{(n-i)z_{i-1}}{\theta}} \left(\frac{1}{\theta} \right) e^{-\frac{(n-i+2)z_{i+1}}{\theta}}\\
& \times \cdots \times \left(\frac{1}{\theta} \right) e^{-\frac{z_n}{\theta}}dz_1\cdots dz_{i-1}dz_{i+1} \cdots dz_{n}\\
&= (n-i+1)\left(\frac{1}{\theta} \right) e^{-\frac{(n-i+1)z_{i}}{\theta}} \int_0^\infty n\left(\frac{1}{\theta} \right) e^{-\frac{nz_1}{\theta}}dz_1 \times \cdots \times \int_0^\infty\left(\frac{1}{\theta} \right) e^{-\frac{(n-i)z_{i-1}}{\theta}}dz_{i-1} \int_0^\infty\left(\frac{1}{\theta} \right) e^{-\frac{(n-i+2)z_{i+1}}{\theta}}dz_{i+1}  \\
&\times \cdots \times \int_0^\infty\left(\frac{1}{\theta} \right) e^{-\frac{z_{n}}{\theta}}dz_{n}.
\end{align*}
Ahora, observemos que cada integral representa la integral de la función de probabilidad de una v.a. con distribución $Exponencial(\frac{n-i}{\theta})$, por lo que podemos simplificarlas a 1 y por lo que podemos concluir que la marginal de $Z_i$ es 
\begin{align*}
f_{Z_i}(z_i)=  \left(\frac{n-i+1}{\theta} \right) e^{-\frac{(n-i+1)z_{i}}{\theta}}, \ \ \ \ \ \ z_i>0.
\end{align*}
Por lo que podemos concluir que $Z_i\sim Exponencial(\frac{\theta}{n-i+1})$, y además si observamos la función de distribución conjunta podemos notar que podemos factorizar todas las $Z_i$ por lo que de igual forma \textbf{podemos concluir que las $Z_i$ son independientes.} Ahora determinaremos la distribución de $A=2(n-i+1)Z_i/\theta$, utilizando la generadora de momentos de una v.a. con distribución exponencial tenemos que
\begin{align*}
M_A(t)=M_{Z_i}(t2(n-i+1)/\theta) =\frac{1}{1-\frac{\theta}{n-i+1}\cdot t2(n-i+1)/\theta}= \frac{1}{1- 2t}.
\end{align*}
Por lo tanto, \textbf{podemos concluir que} $2(n-i+1)Z_i/\theta\sim Exponencial(2).$ \ \ \ \ \fin 

9. En este ejercicio corroborará mediante simulaciones el Teorema Clásico de Límite Central (TCLC).

a) Escriba la siguiente función en R. Simule una muestra  de tamaño $n$ de una variable aleatoria $Exponencial(\lambda)$ y calcule el estadístico $Z_n\equiv \frac{\sqrt{n} \bar{X}_n-\lambda^{-1}}{\lambda^{-1}}$. Repita lo anterior $m$ veces. La función deberá tomar como parámetros $n, m$ y $\lambda$ y regresar un vector de tamaño $m$ conteniendo la muestra de $Z_n$.

\res Creamos una función auxiliar que genere una muestra de tamaño $n$ y creamos la función general que repita este procedimiento $m$ veces y calcule el estadístico $Z_n$ para cada muestra.
```{r}

muestra_exponencial <- function(n,lambda){ # función auxiliar
  muestra <- rexp(n,lambda)          # generamos una muestra tamaño n
  z_n <- (sqrt(n)*(mean(muestra)-1/lambda))/(1/lambda) # calculamos z_n
  z_n
}

exponencial_estadistico <- function(n, m, lambda){ # función general
  muestras_z_n <- c() # repetimos la función auxiliar m veces.
  for (i in 1:m){
    muestras_z_n[i] <- muestra_exponencial(n, lambda)
  }
  muestras_z_n        # retornemos los z_n.
}
```

b) Para $n = 5, 10, 100, 500, 1000, 10000, m = 1000$ y $\lambda=1$, utilice la función del inciso anterior para obtener muestras de $Z_n$. Grafíque las muestras anteriores en un histograma (un histograma para cada $n$). ¿Qué observa? ¿Qué tiene que ver su resultado con el TCLC?

\res Recordemos que el Teorema Central de Limite  nos dice que 
\begin{framed}
    \begin{thmt} 
    Sea $X_1, \cdots, X_n$ un conjunto de v.a.i.i.d con media $\mu$ y varianza $\sigma^2$. Sea $\bar{X}=\sum_{i=1}^nX_i$ entonces
    $$Z_n\equiv \frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma}\rightsquigarrow Z $$
    donde $Z\sim N(0,1).$
    \end{thmt}
\end{framed}
entonces en este problema se espera que los histogramos que se grafiquen se parezcan a la distribución de probabilidad de una normal estandar y este parentezco es más notorio cuando $n$ es más grande. Procedemos a calcular las muestras para los distintos tamaños de $n$,

```{r, message=FALSE, warning=FALSE}
n <- c(5, 10, 100, 500, 1000, 10000) # tamaños de muestras
n_muestras <- c() # inicializamos
z_n_muestras <- c()
set.seed(08081997) # semilla.
for (i in 1:length(n)){ # Generamos las 1000 repeticiones.
  n_muestras <- c(n_muestras, rep(n[i], 1000))
  z_n_muestras <- c(z_n_muestras, exponencial_estadistico(n=n[i], m=1000, lambda=1))
}

muestras <- data.frame(n=factor(n_muestras), z_n=z_n_muestras) # concatenamos

ggplot(muestras, aes(x=z_n,fill=n)) + # graficamos los histogramas
    geom_histogram(bins = 20, color="#e9ecef", alpha=0.6, position = 'identity') +
    facet_wrap(~n,scales = "free_x") +
  labs(title="Simulaciones TCLC caso exponencial.")
```

Se observa visualmente que si se satisface el teorema central del límite, pero para concluir es recomendable realizar pruebas de normalidad.

c) Para cada una de las muestras generadas en el inciso anterior, encuentre el Q-Q plot y el P-P plot normales. Comente sus resultados.

\res Ahora gráficamos los Q-Q plot y P-P plot para determinar si efectivamente el estadístico definido se distribuye como una Normal estandar. 
```{r message=FALSE, warning=FALSE}
library(qqplotr)
ggplot(muestras, aes(sample=z_n)) +
    geom_qq(aes(color=n))+
    labs(title="QQ-plot, estadístico caso exponencial")+
    facet_wrap(~n,scales = "free_y")

ggplot(muestras, aes(sample=z_n)) +
    stat_pp_point(aes(color=n)) +
    labs(title="PP-plot, estadístico caso exponencial")+
    facet_wrap(~n,scales = "free_y")
```

En conclusión, este ejercicio nos ayudar para validar con un conjunto de variables aleatorias con distribución Exponencial con misma media y misma varianza el teorema del limite central. Las graficos de Q-Q y P-P sirvieron para validar este teorema, y claramente se observa que cuando $n$ es más grande las pruebas de normalidad son más notorias. \ \ \ \ \fin 

10. En este ejercicio volverá a trabajar con el TCLC.

a) Escriba una función análoga a la pedida en el inciso 9a) para una distribución Binomial$(p, N)$. La función deberá tomar los mismos parámetros a los pedidos en el inciso 9a), con excepción al parámetro $\lambda$ que tendrá que ser sustituido $p$ y $N$.

\res Para la distribución Binomial el estadístico a calcular en cada muestra estará definido como 
$$ Z_n=\frac{\bar{X}-\mu_X}{\sigma_X/\sqrt{n}}=\frac{\bar{X}-Np}{\frac{\sqrt{Np(1-p)}}{\sqrt{n}}}=\frac{\sqrt{n}(\bar{X}-Np)}{\sqrt{Np(1-p)}}$$
Creamos una función homologa al ejercicio 9.
```{r}
muestra_binomial <- function(n, p, N){
  muestra <- rbinom(n, N, p)
  z_n <- (sqrt(n)*(mean(muestra)-N*p))/(sqrt(N*p*(1-p)))
  z_n
}

binomial_estadistico <- function(n, m, p, N){
  muestras_z_n <- c()
  for (i in 1:m){
  muestras_z_n[i] <- muestra_binomial(n, p, N)
  }
  muestras_z_n
}
```


b) Para $p = 1/2$ y $N = 15$, repita los incisos 9b) y 9c) para el caso Binomial de este ejercicio.

\res Graficamos los histogramas, los Q-Q plot y P-P plot para los distintos tamaños de muestras solicitados. 
```{r}
n <- c(5, 10, 100, 500, 1000, 10000)
n_muestras <- c()
z_n_muestras <- c()
set.seed(08081997)
for (i in 1:length(n)){
  n_muestras <- c(n_muestras, rep(n[i], 1000))
  z_n_muestras <- c(z_n_muestras, binomial_estadistico(n=n[i], m=1000, N=15, p=1/2))
}

muestras <- data.frame(n=factor(n_muestras), z_n=z_n_muestras)

ggplot(muestras, aes(x=z_n,fill=n)) +
    geom_histogram(bins = 20, color="#e9ecef", alpha=0.6, position = 'identity') +
    facet_wrap(~n,scales = "free_x")+
  labs(title="Simulaciones TCLC caso binomial")
```

```{r}
ggplot(muestras, aes(sample=z_n)) +
geom_qq(aes(color=n))+
labs(title="QQ-plot, estadístico caso binomial")+
facet_wrap(~n,scales = "free_y")

ggplot(muestras, aes(sample=z_n)) +
stat_pp_point(aes(color=n)) +
labs(title="PP-plot, estadístico caso binomial")+
facet_wrap(~n,scales = "free_y")
```

Analizando los gráficos podemos concluir que efectivamente se cumple el teorema central del límite, se observa que cuando $n$ es grande la verificación es más notoria.

c) Para $p=0.1, N = 15, n = 5, 10, 20, 100$ y $m = 1000$, genere muestras de $Z_n$ y grafíque estás muestras en un histograma (un histograma para cada n). ¿Qué observa? Explíque.

\res Utilizando lo mismo del inciso anterior solo que haciendo la probabilidad más pequeña y reduciendo los tamaños $n$, tenemos que 
```{r}
n <- c(5, 10, 20, 100)
n_muestras <- c()
z_n_muestras <- c()
set.seed(08081997)
for (i in 1:length(n)){
  n_muestras <- c(n_muestras, rep(n[i], 1000))
  z_n_muestras <- c(z_n_muestras, binomial_estadistico(n=n[i], m=1000, N=15, p=0.1))
}

muestras <- data.frame(n=factor(n_muestras), z_n=z_n_muestras)

ggplot(muestras, aes(x=z_n,fill=n)) +
    geom_histogram(bins = 15, color="#e9ecef", alpha=0.6, position = 'identity') +
    facet_wrap(~n,scales = "free_x")+
  labs(title="Simulaciones TCLC caso binomial (p=0.1)")
```
Con los tamaño propuestos $n$ como no es grande no se ve tan claro que el estadístico se distribuya como una normal estandar.

d) Repita el inciso anterior para $p = 0.99$. Compare su resultado con lo obtenido en el inciso anterior.

\res 
```{r}
n <- c(5, 10, 20, 100)
n_muestras <- c()
z_n_muestras <- c()
set.seed(08081997)
for (i in 1:length(n)){
  n_muestras <- c(n_muestras, rep(n[i], 1000))
  z_n_muestras <- c(z_n_muestras, binomial_estadistico(n=n[i], m=1000, N=15, p=0.99))
}

muestras <- data.frame(n=factor(n_muestras), z_n=z_n_muestras)

ggplot(muestras, aes(x=z_n,fill=n)) +
    geom_histogram(bins = 15, color="#e9ecef", alpha=0.6, position = 'identity') +
    facet_wrap(~n,scales = "free_x")+
  labs(title="Simulaciones TCLC caso binomial (p=0.99)")
```

Si comparmos los resultados observamos en este inciso y los del inciso c) se observa una gran diferencia en la convergencia de la distribución de los estadísticos, esto se debe a que como en este inciso $p$ es cercano a uno los $n$ propuestos no son lo suficientemente grandes para que se observe la similitud con la distribución de probabilidad normal estandar, es decir, $n$ en este inciso tiene que ser más grande para que se observe más claro el teorema central del límite.

11. Sean $X_1$ y $X_2$ v.a.i que tienen una distribución normal estándar. Obtenga la densidad conjunta de $(Y_1,Y_2)$, donde $Y_1=\sqrt{X_1^2+X_2^2}$ y $Y_2 =X_1/X_2$. ¿Son $Y_1$ y $Y_2$ independientes? \\

\res Como $X_1$ y $X_2$ v.a.i que tienen una distribución normal estándar entonces su distribución conjunta es
\begin{align*}
f_{X,Y}(x,y)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}y^2}, \ \ \ -\infty<x<\infty, \ -\infty<y<\infty.
\end{align*}
Ahora, considerando las transformaciones del problema, tenemos que las transformaciones inversas son 
\begin{align*}
X_2=\sqrt{Y_1-X_1^2}, \ \ \ \  X_1= Y_2 X_2 \Rightarrow\\
X_2=\sqrt{Y_1^2-Y_2^2 X_2^2}\Leftrightarrow X_2^2=Y_1^2-Y_2^2 X_2^2\Leftrightarrow X_2=\frac{Y_1}{\sqrt{1+Y_2^2}} \Rightarrow \\
X_1=\frac{Y_1 Y_2}{\sqrt{1+Y_2^2}}=Y_1 \sqrt{\frac{Y_2^2}{Y_2^2+1}}=Y_1 \sqrt{\frac{Y_2^2+1-1}{Y_2^2+1}}=Y_1 \sqrt{1-\frac{1}{Y_2^2+1}}
\end{align*}
Observamos que estas funciones no son uno a uno ya que tenemos el termino $\pm \sqrt{1+Y_2^2}$ por lo que si dividimos el área en dos positivas y negativas haciendo este cambio podemos definir las transformaciones anteriores y las inversas serán uno a uno por lo que no cambiaría la función de probabilidad conjunta, por lo que tenemos el Jacobiano
\begin{align*}
J&=\left|\begin{array}{cc}
\frac{y_2}{\sqrt{y_2+1}} & \frac{y_1}{2\sqrt{1-\frac{1}{y_2^2+1}}}\left(\frac{2y_2}{(y_2^2+1)^2} \right)\\
\frac{1}{\sqrt{y_2^2+1}} & -\frac{2y_1y_2}{(y_2^2+1)^{3/2}}
\end{array} \right|=\left|\begin{array}{cc}
\frac{y_2}{\sqrt{y_2+1}} & \frac{\frac{y_1y_2}{(y_2^2+1)^2}}{\frac{y_2}{\sqrt{y_2^2+1}}}\\
\frac{1}{\sqrt{y_2^2+1}} & -\frac{2y_1y_2}{2(y_2^2+1)^{3/2}}
\end{array} \right|=\left|\begin{array}{cc}
\frac{y_2}{\sqrt{y_2+1}} & \frac{y_1}{\sqrt{y_2^2+1} (y_2^2+1)}\\
\frac{1}{\sqrt{y_2^2+1}} & -\frac{y_1y_2}{(y_2^2+1)^{3/2}}
\end{array} \right|\\
&=-\frac{y_1y_2^2}{(y_2^2+1)^2}-\frac{y_1}{(y_2^2+1)^2}=-\frac{y_1(y_2^2+1)}{(y_2^2+1)^2}\Rightarrow |J| = \frac{y_1}{y_2^2+1}.
\end{align*}
Entonces, \textbf{la densidad conjunta de $(Y_1, Y_2)$ es}
\begin{align*}
f_{Y_1,Y_2}(y_1,y_2)=\frac{2}{\sqrt{2\pi}}e^{-\frac{1}{2}\frac{y_1^2y_2^2}{y^2+1}}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\frac{y_1^2}{y_2^2+1}}\frac{y_1}{y_2^2+1}=\frac{1}{\pi} e^{-\frac{1}{2}y_1^2}\frac{y_1}{y_2^2+1}.\ \ \ \ y>0, -\infty< y_2<\infty. 
\end{align*} 
Ahora calculemos las marginales de $Y_1$ y $Y_2$
\begin{align*}
f_{Y_1}(y_1)=\intim \frac{1}{\pi} e^{-\frac{1}{2}y_1^2}\frac{y_1}{y_2^2+1} dy_2=y_1e^{-\frac{1}{2}y_1^2}\intim \frac{1}{\pi(y_2^2+1)} dy_2
\end{align*}
observemos que la integral representa la integral de la función de probabilidad de una v.a. Cauchy, por lo que equivale a 1 y por lo tanto podemos concluir que la marginal de $Y_1$ es
\begin{align*}
f_{Y_1}(y_1)=y_1e^{-\frac{1}{2}y_2^2}, \ \ \ \ y>0.
\end{align*}
Y la marginal de $Y_2$ seria 
\begin{align*}
f_{Y_2}(y_2)=\intim \frac{1}{\pi} e^{-\frac{1}{2}y_1^2}\frac{y_1}{y_2^2+1} dy_1=\frac{1}{\pi(y_2^2+1)}\intim  y_1e^{-\frac{1}{2}y_1^2} dy_1,
\end{align*}
haciendo un cambio de variable $m=y_1^2\Rightarrow \ dm=2y_1dy_1$ implica que 
\begin{align*}
f_{Y_2}(y_2)&=\int_0^\infty  \frac{1}{\pi} e^{-\frac{1}{2}y_1^2}\frac{y_1}{y_2^2+1} dy_1=\frac{1}{\pi(y_2^2+1)}\int_0^\infty  y_1e^{-\frac{1}{2}y_1^2} dy_1=\frac{1}{\pi(y_2^2+1)}\int_0^\infty  \frac{1}{2}e^{-\frac{1}{2}m} dy_m\\
&=\frac{1}{\pi(y_2^2+1)} \ \ \ \ \ \ \ \ \ \ -\infty< y_2<\infty. 
\end{align*}
Es decir, la marginal de $Y_2$ es
\begin{align*}
f_{Y_2}(y_2)=\frac{1}{\pi(y_2^2+1)} \ \ \ \ \ \ \ \ \ \ -\infty< y_2<\infty.
\end{align*}
Y por lo tanto, como 
\begin{align*}
f_{Y_1,Y_2}(y_1,y_2)=f_{Y_1}(y_1)f_{Y_2}(y_2)
\end{align*}
\textbf{podemos concluir que $Y_1$ y $Y_2$ son independientes.}  

12. Sea $A$ el triángulo de vértices $(0, 0), (0, 1), (1, 0)$ y suponga que $X, Y$ tiene una densidad conjunta uniforme en el triángulo. Halle la distribución marginales de $X+Y$.\\

\res Recordemos que la densidad conjunta de $X, Y$ podemos escribirla como 
\begin{align*}
f_{X,Y}(x,y)= 2,  \ \ \ \ 0\leq x \leq 1-y, 0\leq y \leq 1.
\end{align*}
Entonces ocupemos la transformación 
\begin{align*}
v=x+y, \ \ \ \ \ \text{y}\ \ \ \ u=y
\end{align*}
Entonces, las transformaciones inversas son 
\begin{align*}
x=v-u, \ \ \  \ \ \ \ \ y=u,
\end{align*}
esto último define una transformación uno a uno
\begin{align*}
\chi =\{(x,y)| 0\leq x \leq 1-y, 0\leq y \leq 1\} \Rightarrow \mathcal{Y}= \{(v,u)| u\leq v\leq 1, \ 0\leq u\leq 1 \}.
\end{align*}
y el Jacobiano es
\begin{align*}
J= \left|\begin{array}{cc}
1 & -1\\
0 & 1
\end{array} \right|= 1 \ \Rightarrow |J|=1.
\end{align*}
Entonces la distribución conjunta de $V$ y $U$ es
\begin{align*}
f_{V,U}(v,u)=2, \ \ \ \ \ u\leq v \leq 1, \ \ \ 0\leq u \leq 1.
\end{align*}
que es equivalente a 
\begin{align*}
f_{V,U}(v,u)=2, \ \ \ \ \ 0\leq v \leq 1, \ \ \ 0\leq u \leq v.
\end{align*}
Por lo tanto, la marginal de $V$ es
\begin{align*}
f_V(v)=\int_0^v 2 du= 2v \ \ 0\leq v \leq 1, \ \ \ \ \ \finf
\end{align*}

\textbf{Ejercicios de las notas:}

1.1. Determina qué variables aleatorias corresponden a la generatrices $M_{X,Y}(t,0)$ y $M_{X,Y}(0,t)$.

\res Utilizando las definición de generatrices tenemos que 
\begin{align*}
M_{X,Y}(t,0)=\mE[e^{tx+0y}]=\mE[e^{tx}]=M_{X}(t) \ \ y \ \ \\
M_{X,Y}(0,t)=\mE[e^{0x+ty}]=\mE[e^{ty}]=M_{y}(t)
\end{align*}
Por lo tanto, las variables aleatorias que corresponde a las generatrices $M_{X,Y}(t,0)$ y $M_{X,Y}(0,t)$ son $X$ y $Y$ respectivamente. 

1.2. Si $X$ y $Y$ tienen una distribución normal bivariada, y $U=X+Y, W=X-Y$. Determina el coeficiente de correlación de $U$ y $W$.

\res Cómo $X$ y $Y$ tienen una distribución normal bivarida, esto implica que $X$ y $Y$ son normales independientes. Ahora, utilizando lo anterior y la definición de covarianza tenemos que 
\begin{align*}
Cov(U,W)&=\mE[UW]-\mE[U]\mE[W]\\
&=\mE[(X+Y)(X-Y)]-\mE[X+Y]\mE[X-Y]\\
&= \mE[X^2-Y^2]-(\mE[X]+\mE[Y])(\mE[X]-\mE[Y])\\
&=\mE[X^2]-\mE[Y^2]-\mE[X]^2+\mE[Y]^2\\
&=(\mE[X^2]-\mE[X]^2)-(\mE[Y^2]-\mE[Y]^2)=\sigma_X^2-\sigma_Y^2.
\end{align*}
Ahora calculemos la varianza de ambas variables:
\begin{align*}
Var(U)=Var(X+Y)=Var(X)+Var(Y)=\sigma_X^2+\sigma_Y^2\\
Var(W)=Var(X-Y)=Var(X)-Var(Y)=\sigma_X^2-\sigma_Y^2
\end{align*}
Por lo tanto, el coeficiente de correlación de $U$ y $W$ es
\begin{align*}
\rho = \frac{Cov(U,W)}{\sigma_U\sigma_W}=\frac{\sigma_X^2-\sigma_Y^2}{\sqrt{\sigma_X^2+\sigma_Y^2}\sqrt{\sigma_X^2-\sigma_Y^2}}=\frac{\sqrt{\sigma_X^2-\sigma_Y^2}}{\sqrt{\sigma_X^2+\sigma_Y^2}}=\frac{\sigma_W}{\sigma_U}.\ \ \ \finf
\end{align*}

2.1. Demostrar que si $X\sim t_n$, entonces la distribución de la v.a $Y=X^2$ es una $F_{1,n}$.

\res Utilizando la metodología de la función de distribución acumulada tenemos que 
\begin{align*}
F_Y(y)=\mP(Y<y)=\mP(X^2<y)=\mP(-\sqrt{y}\leq X \leq \sqrt{y})=F_X(\sqrt{y})-F_X(-\sqrt{y}) \ \ y>0.
\end{align*}
Derivando podemos encontrar la densidad de $Y$. Aplicando la regla de la cadena,
\begin{align*}
f_Y(y)=F'_y(y)=\frac{1}{2\sqrt{y}} f_X(\sqrt{y})+\frac{1}{2\sqrt{y}}f_X(-\sqrt{y})=\frac{1}{2\sqrt{y}}(f_X(\sqrt{y})+f_X(-\sqrt{y})). \ \ y>0.
\end{align*}
Entonces como $X$ se distribuye como una variable $t_v$ tenemos
\begin{align*}
f_Y(y)&=\frac{1}{2\sqrt{y}}(f_X(\sqrt{y})+f_X(-\sqrt{y}))=\frac{1}{2\sqrt{y}}\left(\frac{\Gamma\left(\frac{n+1}{2} \right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2} \right) (1+\frac{\sqrt{y}^2}{n})^{(n+1)/2}}+\frac{\Gamma\left(\frac{n+1}{2} \right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2} \right) (1+\frac{(-\sqrt{y})^2}{n})^{(n+1)/2}}\right)\\
&=\frac{1}{\sqrt{y}}\left(\frac{\Gamma\left(\frac{n+1}{2} \right)}{\sqrt{n\pi}\Gamma\left(\frac{n}{2} \right) (1+\frac{y}{n})^{(n+1)/2}}\right)=\left(\frac{\Gamma\left(\frac{n+1}{2} \right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{n}{2} \right)}\right)\frac{1}{\sqrt{n}}\frac{1}{(1+\frac{y}{n})^{(n+1)/2}}\frac{1}{\sqrt{y}}\\
&=\frac{\Gamma\left(\frac{n+1}{2} \right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{n}{2} \right)}\left(\frac{1}{n}\right)^{1/2}y^{1/2-1}\left(1+\frac{1}{n}y \right)^{-(n+1)/2} \ \y>0,
\end{align*}
es decir, la función de distribución de $Y=X^2$ es
\begin{align*}
\fy=\frac{\Gamma\left(\frac{n+1}{2} \right)}{\Gamma\left(\frac{1}{2}\right)\Gamma\left(\frac{n}{2} \right)}\left(\frac{1}{n}\right)^{1/2}y^{1/2-1}\left(1+\frac{1}{n}y \right)^{-(n+1)/2} \ \y>0.
\end{align*}
Y por lo tanto, podemos concluir que $Y$ se distribuye como una $F_{1,n}$. \ \ \ \fin

2.2  En el último ejemplo considera la división $\frac{1}{F}=R=\frac{\frac{Y}{m}}{\frac{X}{n}}$ y demuestra que $R\sim F_{m,n}$ una $F$ con los grados de libertad intercambiados.

\res Cómo $X\sim \chi^2_n, Y\sim \chi^2_m$ independientes, entonces tenemos que la distribución conjunta es el producto de sus densidades, es decir,
\begin{align*}
f_{X,Y}(x,y)=\frac{1}{\Gamma \left(\frac{n}{2}\right)2^{\frac{n}{2}}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}\frac{1}{\Gamma \left(\frac{m}{2}\right)2^{\frac{m}{2}}}y^{\frac{m}{2}-1}e^{-\frac{y}{2}}, \ \ \ x,y>0.
\end{align*}
Ahora, como estamos interesados en encontrar la densidad de  $R=\frac{\frac{Y}{m}}{\frac{X}{n}}$ realicemos la metodología de cambio de variable para encontrarla. Si hacemos 
\begin{align*}
V=\frac{nY}{mX}, \ \ \ U=X,
\end{align*}
la transformación inversa es
\begin{align*}
y=\frac{muv}{n}, \ \ x=u, \ \ \ u, v>0.
\end{align*}
Esto último define una transformación uno a uno
\begin{align*}
\chi =\{(x, y)| x\in \mR^+, y\in \mR^+\}\Rightarrow \mathcal{Y}=\{(v, u)| v\in \mR^+, u\in \mR^+ \}.
\end{align*}
con Jacobiano
\begin{align*}
J=\left|\begin{array}{cc}
\frac{m}{n}u & \frac{m}{n}v\\
0 & 1
\end{array} \right|= \frac{m}{n}u\neq 0.
\end{align*}
Entonces, 
\begin{align*}
f_{U,V}(u,v)&=f_{X,Y}(u, \frac{muv}{n})|J|\\
&=\frac{1}{\Gamma \left(\frac{n}{2}\right)2^{\frac{n}{2}}}u^{\frac{n}{2}-1}e^{-\frac{u}{2}}\frac{1}{\Gamma \left(\frac{m}{2}\right)2^{\frac{m}{2}}}\left(\frac{muv}{n}\right)^{\frac{m}{2}-1}e^{-\frac{\left(\frac{muv}{n}\right)}{2}}\frac{mu}{n},\\
&=\frac{mu}{n}\frac{1}{\Gamma \left(\frac{n}{2}\right)\Gamma \left(\frac{m}{2}\right)2^{\frac{m}{2}+\frac{n}{2}}}u^{\frac{n}{2}-1}e^{-\frac{u}{2}}\left(\frac{muv}{n}\right)^{\frac{m}{2}-1}e^{-\frac{muv}{2n}}, \ \ \ u,v>0.
\end{align*}
Nos interesa la marginal de $V$, ya que $V=R=\frac{\frac{Y}{m}}{\frac{X}{m}}$. Luego integramos para todo valor de $U$
\begin{align*}
f_R(v)&=\int_0^\infty f_{U,V}(u,v)du\\
&=\frac{\left(\frac{n}{m}\right)^{m/2}v^{m/2-1}}{\Gamma \left(\frac{n}{2}\right)\Gamma \left(\frac{m}{2}\right)2^{\frac{m}{2}+\frac{n}{2}}}\int_0^\infty u^{\frac{n+m}{2}-1}e^{-u\left(\frac{mv}{2n}+\frac{1}{2} \right)}du
\end{align*}
Recordando que para una v.a. $G\sim Gamma(\alpha, \beta)$ se tiene
\begin{align*}
\int_0^\alpha Gamma(\alpha, \beta)dg=\int_0^\infty\frac{1}{\Gamma(\alpha)\beta^\alpha}g^{\alpha-1}e^{-\frac{g}{\beta}}dg=1,
\end{align*}
entonces
\begin{align*}
\int_0^\infty g^{\alpha-1}e^{-\frac{g}{\beta}}dg=\Gamma(\alpha)\beta^\alpha.
\end{align*}
Entonces, siguiente con la expresión de $f_R(v)$ podemos hacer un cambio de variable para utilizar la propiedad anterior. Sea
\begin{align*}
\alpha=\frac{n+m}{2}, \ \ \ \beta=\left(\frac{mv}{2n}+\frac{1}{2} \right)^{-1}
\end{align*}
entonces 
\begin{align*}
f_R(v)&=\frac{\left(\frac{n}{m}\right)^{m/2}v^{m/2-1}}{\Gamma \left(\frac{n}{2}\right)\Gamma \left(\frac{m}{2}\right)2^{\frac{m}{2}+\frac{n}{2}}}\int_0^\infty u^{\alpha-1}e^{\frac{-u}{\beta}}du=\frac{\left(\frac{n}{m}\right)^{m/2}v^{m/2-1}}{\Gamma \left(\frac{n}{2}\right)\Gamma \left(\frac{m}{2}\right)2^{\frac{m}{2}+\frac{n}{2}}}\Gamma\left(\frac{m+n}{2}\right)\left(\frac{mv}{2n}+\frac{1}{2} \right)^{-\frac{n+m}{2}}\\
&=\frac{\Gamma\left(\frac{m+n}{2}\right)}{\Gamma \left(\frac{n}{2}\right)\Gamma \left(\frac{m}{2}\right)}\left(\frac{n}{m}\right)^{m/2}v^{m/2-1}\left(\frac{mv}{n}+1 \right)^{-\frac{n+m}{2}} ,  \ \ \ \ v>0.
\end{align*}
Por lo tanto, podemos concluir que $R\sim F_{m,n}$, y además se comprueba que es una $F$ con los grados de libertad intercambiados.\ \ \ \ \fin

3. Generar 500 muestras aleatorias de tamaños 5, 10, 100, 500, 1000 de una población $N(0,1)$.

3.1 En cada caso, calcular $\bar{X}$ para cada muestra.

3.2 Graficar su histograma, así como su Q-Q plot. En todos los casos deberá verse como una normal pues esa es la distribución exacta de esos valores. Las imperfecciones del ajuste que puedes notar se deben a que estás haciendo una simulación, sólo estás viendo 500 de un $\#$infinito de posibles resultados.

\res Creemos una función para simular las muestras y determinar $\bar{X}$
```{r}
muestra_normal <- function(n){ # función auxiliar
  muestras <- rnorm(n)        # muestra de tamaño n.
  x_bar <- mean(muestras)    # estadísticco
  x_bar
}
normal_estadisticos <- function(n, m){ # función general
  muestras_x_bar <- c() # repetimos las muestra m veces.
  for(i in 1:m) {
    muestras_x_bar[i] <- muestra_normal(n)
  }
  muestras_x_bar
}
```

Con la función anterior generamos las muestras y calculamos $\bar{X}$
```{r}
n <- c(5, 10, 100, 500, 1000)
n_muestras <- c()
z_n_muestras <- c()
set.seed(08081997)
for (i in 1:length(n)){
n_muestras <- c(n_muestras, rep(n[i], 500))
z_n_muestras <- c(z_n_muestras, normal_estadisticos(n=n[i], m=500))
}
muestras <- data.frame(n=factor(n_muestras), z_n=z_n_muestras)
```
Graficamos los histogramos para cada $n$
```{r}
ggplot(muestras, aes(x=z_n,fill=n)) +
geom_histogram(bins = 20, color="#e9ecef", alpha=0.6, position = "identity") +
facet_wrap(~n,scales = "free_x")+
  labs(title="Simulaciones X_bar Normales")
```
Ahora graficamos los Q-Q plot
```{r}
ggplot(muestras, aes(sample=z_n)) +
  geom_qq(aes(color=n))+
  labs(title="QQ-plot")+
  facet_wrap(~n,scales = "free_y")
```

Observando los gráficos Q-Q Plot, P-P plot podemos concluir que en efecto $\bar{X}$ se distribuye como una Normal, esto se demostró en clase que si la población en donde se simula es una normal estandar entonces $\bar{X}$ igual es una normal con parametros $\mu$ y $\frac{\sigma^2}{n}$, es decir, tiene fundamentos teóricos en donde que lo demuestran. \ \ \ \fin

4. Repetir el ejercicio anterior para $S=\sqrt{\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X})^2}$ la desviación estándar, y concluye.

\res Utilizando los mismos pasos que el inciso anterior. Creemos una función para simular las muestras y determinar la desviación estandar.
```{r}
muestra_normal <- function(n){
  muestras <- rnorm(n)
  x_bar <- sqrt(var(muestras))
  x_bar
}
normal_estadisticos <- function(n, m){
  muestras_std <- c()
  for(i in 1:m) {
    muestras_std[i] <- muestra_normal(n)
  }
  muestras_std
}
```

Generamos las muestras y calculamos $\bar{X}$
```{r}
n <- c(5, 10, 100, 500, 1000)
n_muestras <- c()
z_n_muestras <- c()
set.seed(08081997)
for (i in 1:length(n)){
n_muestras <- c(n_muestras, rep(n[i], 500))
z_n_muestras <- c(z_n_muestras, normal_estadisticos(n=n[i], m=500))
}
muestras <- data.frame(n=factor(n_muestras), z_n=z_n_muestras)
```
Graficamos los histogramos para cada $n$
```{r}
ggplot(muestras, aes(x=z_n,fill=n)) +
geom_histogram(bins = 20, color="#e9ecef", alpha=0.6, position = "identity") +
facet_wrap(~n,scales = "free_x")+
  labs(title="Simulaciones Desviación Muestral Normales")
```
Ahora graficamos los Q-Q plot
```{r}
ggplot(muestras, aes(sample=z_n)) +
  geom_qq(aes(color=n))+
  labs(title="QQ-plot")+
  facet_wrap(~n,scales = "free_y")
```

De igual manera que el inciso anterior, podemos concluir que en efecto la desviación estandar se distribuye como una Normal, esto igual se demostró en clase. \ \ \ \fin

5. Cuando $X\sim N(0, 1)$ y $Y = X^2$ , sabemos que $Y$ tiene una distribución $\chi_1^2$ . Aplica la técnica anterior notando que $Y$ es decreciente para valores negativos de $X$ y creciente para los valores positivos de $X$ -es decir, que será necesario partir la región y sumar las contribuciones en ambas regiones. Es decir que, si $x_i=w_i(y)$,
$$f_Y(y)=\sum_{i=1}^kf(x_i)\left|\frac{dx_i}{dy} \right|$$
la derivada de la $i-$ésima transformación inversa que tengamos que calcular de las $k$ regiones).

\res Tenemos que $Y=X^2$, esto implica que $X_1=\sqrt{Y}$ para $X>0$ y $X_1=-\sqrt{Y}$ para $X<0$. Por lo que para calcular la densidad de $Y$ tenemos 
\begin{align*}
\fy&=f_X(\sqrt{y})\left|\frac{1}{2\sqrt{y}} \right|+f_X(-\sqrt{y})\left|-\frac{1}{2\sqrt{y}} \right|=\frac{1}{2\sqrt{y}}\left[\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}y}+ \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}y}\right]\\
&=\frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}e^{-\frac{1}{2}y}, \ \ \ \ \ \ \ \ y>0
\end{align*}
Por lo tanto, $X^2$ tiene una distribución $\chi_1^2$. \ \ \ \fin 

6. Sean $Z_1,Z_2\sim N(0,1)$ v.a. independientes. Además, sean $\mu_1, \mu_2\in \mR, \ \rho \in (-1,1)$ y $\sigma_1,\sigma_2\in (0,\infty).$ Consideremos la transformación dada por
$$X=\mu_1+\sigma_1Z_1, \ \ \ Y=\mu_2+\sigma_2[\rho Z_1+\sqrt{1-\rho^2}Z_2].$$

Demuestra que $X\sim N(\mu_1, \sigma_1)$ y $Y\sim N(\mu_2, \sigma_2)$. Además, usa el teorema de la transformación de variables para demostrar que la densidad de $(X,Y)$ esta dada por 

$$f_{X,Y}(x,y)=\frac{\exp\left(-\frac{1}{2(1-\rho^2)}\left\{ \left( \frac{x-\mu_x}{\sigma_x}\right)^2-2\rho\left( \frac{x-\mu_x}{\sigma_x}\right)\left( \frac{y-\mu_y}{\sigma_y}\right) +\left( \frac{y-\mu_y}{\sigma_y}\right)^2 \right\} \right)}{2\pi\sigma_x \sigma_y\sqrt{1-\rho^2}}$$
para $x,y\in \mR$.

\res Tenemos que la conjunta de $Z_1, Z_2$ por ser independientes es
\begin{align*}
f_{Z_1,Z_2}(z_1,z_2)=\frac{1}{2\pi}\exp\left(-\frac{z_1^2+z_2^2}{2} \right).
\end{align*}
Entonces, tenemos que las transformaciones inversas son
\begin{align*}
z_1=\frac{x-\mu_1}{\sigma_1}, \ \ z_2=\frac{y-\mu_2-\sigma_2\rho z_1}{\sigma_2\sqrt{1-\rho^2}}=\frac{y-\mu_2-\sigma_2\rho \frac{x-\mu_1}{\sigma_1}}{\sigma_2\sqrt{1-\rho^2}}
\end{align*}
Entonces,
\begin{align*}
J= \left|\begin{array}{cc}
\frac{1}{\sigma_1} &0\\
\frac{\rho}{\sigma_1\sqrt{1-\rho^2}} & \frac{1}{\sigma_2\sqrt{1-\rho^2}}
\end{array} \right|= \frac{1}{\sigma_1\sigma_2\sqrt{1-\rho^2}}\Rightarrow |J|=\frac{1}{\sigma_1\sigma_2\sqrt{1-\rho^2}}.
\end{align*}
Así, 
\begin{align*}
f_{X,Y}(x,y)&=\frac{1}{2\pi}\exp\left(-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2+\left(\frac{y-\mu_2-\sigma_2\rho \frac{x-\mu_1}{\sigma_1}}{\sigma_2\sqrt{1-\rho^2}} \right)^2}{2} \right)\frac{1}{\sigma_1\sigma_2\sqrt{1-\rho^2}}\\  
&=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{(1-\rho^2)\left(\frac{x-\mu_1}{\sigma_1}\right)^2+\left(\frac{y-\mu_2}{\sigma_2}-\rho\frac{x-\mu_1}{\sigma_1}\right)^2}{2(1-\rho^2)} \right)\\
&=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2+\rho^2\left(\frac{x-\mu_1}{\sigma_1}\right)^2+
\left(\frac{y-\mu_2}{\sigma_2}\right)^2-
2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)-
\left(\rho\frac{x-\mu_1}{\sigma_1}\right)^2}{2(1-\rho^2)} \right)\\
&=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2-
2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)+
\left(\frac{y-\mu_2}{\sigma_2}\right)^2}{2(1-\rho^2)} \right).
\end{align*}
Es decir, \textbf{la densidad conjunta de $X$ y $Y$ es}
\begin{align*}
f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2-2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)+
\left(\frac{y-\mu_2}{\sigma_2}\right)^2}{2(1-\rho^2)} \right).
\end{align*}
Ahora calculemos la marginales de $X$ y $Y$. Empecemos calculando la marginal de $X$, para facilitar la notación observemos que $u=\frac{x-\mu_1}{\sigma_1}$ y $v=\frac{y-\mu_2}{\sigma_2}\Rightarrow dv=\frac{1}{\sigma_2}dy,$ entonces
\begin{align*}
\fx &= \intim \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2-2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)+
\left(\frac{y-\mu_2}{\sigma_2}\right)^2}{2(1-\rho^2)} \right) dy\ (\text{def. marginal})\\
&=\intim \frac{1}{2\pi\sigma_1\sqrt{1-\rho^2}}\exp\left(-\frac{u^2-2\rho uv+v^2}{2(1-\rho^2)} \right)dv\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ (\text{cambio de variable})\\
&=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{u^2}{2(1-\rho^2)} \right)\intim\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^2}} \exp\left(-\frac{-2\rho uv+v^2}{2(1-\rho^2)} \right)dv\ \ \ \ (\text{separamos terminos})\\
&=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{u^2}{2(1-\rho^2)}-\frac{p^2u^2}{2(1-\rho^2)} \right)\intim\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2} \left(\frac{v-pu}{\sqrt{(1-\rho^2)}} \right)^2\right)dv\ \ (\text{t.c.p})\\
&=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{1}{2}\left[\frac{u^2-p^2u^2}{(1-\rho^2)} \right]\right)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (\text{integral de la densidad de una normal})\\
&=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{1}{2}u^2\right)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2\right). \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (\text{simplificamos})
\end{align*}
\textbf{Por lo tanto, $X\sim Normal(\mu_1, \sigma_1^2)$}. Ahora, para calcular la marginal de $Y$ se realiza un procedimiento análogo $u=\frac{x-\mu_1}{\sigma_1}\Rightarrow du=\frac{1}{\sigma_1}dx$ y $v=\frac{y-\mu_2}{\sigma_2},$ entonces
\begin{align*}
\fx &= \intim \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2-2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right)+
\left(\frac{y-\mu_2}{\sigma_2}\right)^2}{2(1-\rho^2)} \right) dx\ (\text{def. marginal})\\
&=\intim \frac{1}{2\pi\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac{u^2-2\rho uv+v^2}{2(1-\rho^2)} \right)du\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ (\text{cambio de variable})\\
&=\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{v^2}{2(1-\rho^2)} \right)\intim\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^2}} \exp\left(-\frac{-2\rho uv+u^2}{2(1-\rho^2)} \right)du\ \ \ \ (\text{separamos terminos})\\
&=\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{f^2}{2(1-\rho^2)}-\frac{p^2v^2}{2(1-\rho^2)} \right)\intim\frac{1}{\sqrt{2\pi}\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2} \left(\frac{u-pv}{\sqrt{(1-\rho^2)}} \right)^2\right)du\ \ (\text{t.c.p})\\
&=\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{1}{2}\left[\frac{v^2-p^2v^2}{(1-\rho^2)} \right]\right)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  (\text{integral de la densidad de una normal})\\
&=\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{1}{2}v^2\right)=\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{1}{2}\left(\frac{y-\mu_2}{\sigma_2}\right)^2\right).\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (\text{simplificamos})
\end{align*}
Por lo tanto, \textbf{hemos demostrado también que $Y\sim Normal(\mu_2, \sigma_2^2)$}. \ \ \ \ \ \fin

7.1 Sean $U_1\sim Unif(0,1)$ y $U_2\sim Unif(0,1)$ independientes. Encontrar la función de densidad conjunta de $X=\sqrt{-2ln(U_1)} \cos(2\pi U_2)$ y $Y=\sqrt{-2ln(U_1)}\sen(2\pi U_2)$ mediante el teorema de la transformación para dos variables y verificar que $X$ y $Y$ son variables aleatorias $N(0,1)$ independientes.

\res Ocupemos el teorema de la transformación para dos variables, pero encontremos la densidad marginal de dos variables auxiliares y probaremos que son independientes, para poder ocuparlas para encontrar la marginal de $X$ y $Y$ y probar que son independientes.\\

Sea $U=-\beta \ln (U_1)$ y $V=2\pi U_2$. Entonces como la densidad conjunto de $U_1$ y $U_2$ es (por independencia)
\begin{align*}
f_{U_1,U_2}(u_1,u_2)=f_{U_1}(u_1)f_{U_2}(u_2)=1 \ \ 0< u_1< 1, \ \ 0< u_2< 1.
\end{align*}
Si la transformación es 
\begin{align*}
u=-\beta \ln (u_1) \ \ \text{y si} \ \ v=2\pi u_2
\end{align*}
entonces la transformación inversa es
\begin{align*}
u_1=\exp\left(-\frac{u}{\beta}\right), \ \ \ u_2=\frac{v}{2\pi}
\end{align*}
Entonces observemos que 
\begin{align*}
\chi =\{(u_1, u_2)| u_1\in (0,1), u_2\in (0,1) \}\Rightarrow \mathcal{Y}=\{(u, v)| u\in \mR^+, v\in (0,2\pi) \}.
\end{align*}
Entonces,
\begin{align*}
J=\left| \begin{array}{cc}
-\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right)& 0\\
0 & \frac{1}{2\pi}
\end{array} \right\|=-\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) \frac{1}{2\pi} \Rightarrow |J|=\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) \frac{1}{2\pi} 
\end{align*}
Así, 
\begin{align*}
f_{U,V}(u,v)= \frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) \frac{1}{2\pi} \ \ \ 0<u<\infty, \ 0<v<2\pi.
\end{align*}
Entonces, podemos calcular las marginales de $U$ y $V$
\begin{align*}
&f_{U}(u)= \int_{0}^{2\pi}\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) \frac{1}{2\pi}dv=\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) \frac{1}{2\pi}\cdot\left. v\right|_{v=0}^{v=2\pi}=\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right).\\
&f_{V}(v)= \int_{0}^{\infty}\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) \frac{1}{2\pi}du=\frac{1}{2\pi}\int_{0}^{\infty}\frac{1}{\beta} \exp\left(-\frac{u}{\beta}\right) du=\frac{1}{2\pi}.
\end{align*}
Por lo anterior podemos concluir que $U\sim \exp(\beta)$ y $V\sim Unif(0,2\pi)$, y es sencillo ver que $f_{U,V}(u,v)=f_{U}(u)\cdot f_{V}(v)$ por lo que $U$ y $V$ son independientes. Nuevamente ocuparemos el teorema de transfromación para dos variables para encontrar determinar las marginales de dos transformaciones de $U$ y $V$. \\

Sea $U=X^2+Y^2$ y $V=\tan^{-1}\left(\frac{Y}{X} \right)$,  entonces la transformación inversa es $X=\sqrt{-2ln(U_1)} \cos(2\pi U_2)=\sqrt{U}\cos(V)$ y $Y=\sqrt{-2ln(U_1)}\sen(2\pi U_2)=\sqrt{U}\sen(V),$
\begin{align*}
\chi =\{(u, v)| u\in \mR^+, v\in (0,2\pi) \}\Rightarrow \mathcal{Y}=\{(x, y)| x\in \mR, y\in \mR \}.
\end{align*}
Entonces 
\begin{align*}
J=\left|\begin{array}{cc}
\frac{\cos(u)}{2\sqrt{u}} & -\sqrt{u}\sen(v)\\
\frac{\sen(u)}{2\sqrt{u}} & \sqrt{u}\cos(v)
\end{array} \right|=\frac{\sqrt{u}}{2\sqrt{u}}\cos^2(u)+\frac{\sqrt{u}}{2\sqrt{u}}\sen^2(u)=\frac{1}{2}\ \Rightarrow |J|=\frac{1}{2}.
\end{align*} 
Por lo tanto la densidad conjunta de $X$ y $Y$ es
\begin{align*}
f_{X,Y}(x,y)= \exp\left(-\frac{x^2+y^2}{2}\right) \frac{1}{2\pi} \ \ \ -\infty<x<\infty, \  -\infty<y<\infty.
\end{align*}
Entonces la marginales de $X$ y $Y$ son
\begin{align*}
\fx &= \intim \exp\left(-\frac{x^2+y^2}{2}\right) \frac{1}{2\pi} dy =  \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2} \right)\intim \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{y^2}{2}\right) dy\\
&= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2} \right),\ \ -\infty <x<\infty. 
\end{align*}
\begin{align*}
\fy &= \intim \exp\left(-\frac{x^2+y^2}{2}\right) \frac{1}{2\pi} dx =  \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{y^2}{2} \right)\intim \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right) dy\\
&= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{y^2}{2} \right),\ \ -\infty <y<\infty. 
\end{align*}
Por lo tanto, \textbf{podemos concluir que $X\sim N(0,1)$ y $Y\sim N(0,1)$, y además como $f_{X,Y}(x,y)=\fx \fx$ podemos concluir que son independientes}.

7.2 Usar este resultado para generar 100 valores de una v.a. $N(3,16)$ y hacer un histograma con los valores obtenidos. ¿Podrías hacer un Q-Q plot para estos datos?

\res Creemos una función para generar 100 valores de una v.a. $N(3,16)$, para ello observemos que sea $Z_1\sim N(0,1)$ entonces es fácil mostrar que $W=3+4Z_1$ es igual a una $N(3,16)$, ya que 
$$f_W(w)=\frac{1}{\sqrt{2\pi}}\exp{-\frac{1}{2}\left(\frac{w-3}{4} \right)^2}\frac{1}{4}= \frac{1}{4\sqrt{2\pi}}\exp{-\frac{1}{2}\left(\frac{w-3}{4} \right)^2}$$
Ocupando lo anterior podemos crear la función para generar $n$ números aleatorias de una normal estandar.
```{r}
random_normal <- function(n){
  u1 <- runif(n, 0, 1)
  u2 <- runif(n, 0, 1)
  number_random <- sqrt(-2*log(u1))*sin(2*pi*u2)
  number_random
}
```

Ocupando la función anterior generamos números aleatorios de una $N(3,16)$ y graficamos el histograma.

```{r message=FALSE, warning=FALSE}
norm_random <- random_normal(100) 

norm_mean_3_var_16 <- 3+4*norm_random

ggplot(data= data.frame(x=norm_mean_3_var_16), aes(x))+
  geom_histogram(nbins=10, fill="blue")+
  labs(title="Histogramas números aleatorios.")
```

Como son pocos números aleatorios no se puede observer claramente que sea una distribución normal $N(3,16)$, realizamos el Q-Q plot

```{r}
ggplot(data= data.frame(x=norm_mean_3_var_16), aes(sample=x))+
  geom_qq(color="blue")
```

Bueno, con esta gráfica Q-Q plot observamos que si se distribuye como una normal. Para ver este hecho más claro realizamos lo anterior pero generando 10000 números aleatorios.

```{r message=FALSE, warning=FALSE}
norm_random <- random_normal(10000) 

norm_mean_3_var_16 <- 3+4*norm_random

ggplot(data= data.frame(x=norm_mean_3_var_16), aes(x))+
  geom_histogram(nbins=10, fill="blue")+
  labs(title="Histogramas números aleatorios.")
```

En este histograma es más claro que se se realizo bien la generación de números aleatorios. \fin