\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{xspace}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{float}
\usepackage{hyperref}
\usepackage{color}
\usepackage{framed}

%\usepackage[colorlinks=true, linkcolor=black, urlcolor=blue, pdfborder={0 0 0}]{hyperref}

\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\newcommand{\X}{\mathbb{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\xbarn}{\bar{x}_n}
\newcommand{\ybarn}{\bar{y}_n}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\llaves}[1]{\left\lbrace #1 \right\rbrace}
\newcommand{\barra}{\,\vert\,}
\newcommand{\mP}{\mathbb{P}}
\newcommand{\mE}{\mathbb{E}}

\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\muv}{\boldsymbol{\mu}}
\newcommand{\mcov}{\boldsymbol{\Sigma}}
\newcommand{\vbet}{\boldsymbol{\beta}}
\newcommand{\veps}{\boldsymbol{\epsilon}}
\newcommand{\mC}{\mathbf{C}}
\newcommand{\ceros}{\boldsymbol{0}}
\newcommand{\mH}{\mathbf{H}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\avec}{\mathbf{a}}
\newcommand{\res}{\textbf{RESPUESTA}\\}

\newcommand{\fin}{$\blacksquare.$}
\newcommand{\finf}{\blacksquare.}

\newtheorem{thmt}{Teorema:}
\newtheorem{thmd}{Definición:}
\newtheorem{thml}{Lema:}
\newcommand{\intim}{\int_{-\infty}^\infty}
\newcommand{\fx}{f_X(x)}
\newcommand{\fy}{f_Y(y)}

\begin{document}
\begin{table}[ht]
\centering
\begin{tabular}{c}
\textbf{Maestría en Computo Estadístico}\\
\textbf{Inferencia Estadística} \\
\textbf{Tarea 4}\\
\today \\
\emph{Enrique Santibáñez Cortés}\\
Repositorio de Git: \href{https://github.com/Enriquesec/Inferencia_Estad-stica/tree/master/Tareas/Tarea_4}{Tarea 4, IE}.
\end{tabular}
\end{table}
Escriba de manera concisa y clara sus resultados, justificando los pasos necesarios. Serán descontados puntos de los ejercicios mal escritos y que contenga ecuaciones sin una estructura gramatical adecuada. Las conclusiones deben escribirse en el contexto del problema. Todos los programas y
simulaciones tienen que realizarse en R.
%Problema 1
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------

1. Sea $X$ una v.a continua cuya función de distribución $F$ es estrictamente creciente. El Teorema
de la Transformación Integral nos dice que $Y=F(X)$ tiene distribución Uniforme(0, 1).\\

a) Sea $U\sim Uniforme(0, 1)$ y $X'=F ^{-1}(U)$. Muestre que $X' \sim F$.

\res Como $U$ tiene una distribución $Unif(0,1)$, esto implica que $F_U(u)=u$ (por definición de la función acumulada de una distribución uniforme). Entonces tenemos que 
\begin{align*}
F_{X'}(x')=\mP(X'\leq x')=\mP(F^{-1}(U)\leq x')= \mP(U\leq F(x'))= F_U(F(x'))=F(x').
\end{align*}
Por lo tanto, $X'\sim F.$

%Problema 2
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
2. Sea A el triángulo de vértices (0, 0), (0, 1), (1, 0) y suponga que $X, Y$ tiene una densidad conjunta uniforme en el triángulo. (a) Halle las distribuciones marginales de $X, Y \ \text{y}\ Z =X + Y$ . (b) ¿Son $X$ y $Y$ independientes? ¿Por qué?

\res Por como esta descrita la densidad conjunta de $X,Y$ podemos escribirla como 
\begin{align*}
f(x,y)=\left\{\begin{array}{cc}
2,& 0\leq x\leq 1, x\leq y\leq 1\\
0,& \text{otro caso}.
\end{array} .\right.
\end{align*}
Validamos que efectivamente cumpla las condiciones de densidad conjunta, observemos que se cumple que $f(x,y)\geq 0, \ \forall (x,y).$ y además que integra 1:
\begin{align*}
\intim \intim f(x,y)dydx=\int_0^1 \int_x^1 2 dydx=\int_0^1 2(1-x) dx=\left.2x-\frac{2x^2}{2}\right|_0^1=1.
\end{align*}
La definición de distribuciones marginales:
\begin{framed}
    \begin{thmd} \label{marginales}
	Las distribuciones marginales de v.a continuas $X$ y $Y$, denotadas por $f_X(x)$ y $f_Y(y),$ se definen como
	\begin{align*}
	f_X(x) = \intim f(x,y)dy,\\
	\text{y}\\
	f_Y(y)=\intim f(x,y)dx.
	\end{align*}
    \end{thmd}
\end{framed}
Entonces \textbf{a) las marginales son}:
\begin{align*}
f_X(x)=\intim f(x,y)dy =\int_x^1 2 dy=\left. y\right|_{0}^1=2(1-x).\\
f_Y(y)=\intim f(x,y)dx=\int_y^1 2 dx=\left. x\right|_{0}^1=2(1-y),
\end{align*} es decir,
\begin{align*}
f_X(x)=\left\{\begin{array}{cc}
2(1-x) & 0\leq x\leq 1\\
0 & \text{en otro caso.}
\end{array} \right.\ \ \ \ \text{y} \ \ \ f_Y(y)=\left\{\begin{array}{cc}
2(1-y) & 0\leq y\leq 1\\
0 & \text{en otro caso.}
\end{array} \right.
\end{align*} 
\textbf{La marginal de $Z=X+Y$ se entrega en la próxima tarea.}\\

Ocupando la siguiente definición (vista en clase):
\begin{framed}
    \begin{thmd} \label{independencia}
    Dos variables aleatorias continuas se dice que son \textbf{independientes} si su distribución conjunta se puede expresar como el producto de sus marginales.
    $$f(x,y)=f_X(x)f_Y(y).$$
    \end{thmd}
\end{framed} 
Lo anterior es equivalente a decir que si la distribución conjunta no se puede expresar como el producto de sus marginales entonces no es independientes (equivalencia lógica). Entonces observemos que:
\begin{align*}
f(x,y)=2\neq 2(1-x)\cdot 2(1-y)=\fx \fy .
\end{align*}
Por lo tanto, \textbf{b) podemos concluir que $X$ y $Y$ no son independientes por que el producto de sus marginales no es la conjunta.}\ \ \  \fin
 
%Problema 3
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------
3. Halle la densidad condicional de $X|Y=y$ si $(X,Y)$ tiene densidad conjunta \begin{align*}
f_{X,Y}(x,y)=\frac{1}{y}\exp \left(-\frac{x}{y}-y \right) \ \text{para} \ x,y>0.
\end{align*} 
También calcule $\mE(X|Y=y)$.

\res Recordemos la definición de distribución condicional:
\begin{framed}
    \begin{thmd} \label{distribucion_condicional}
    Las distribuciones condicionales de $X|Y=y$ y $Y|X=x$ denotadas por $f_{X|y}(x)$ y $f_{Y|x}(y)$ se definen de la siguiente forma. Para cada valor de $y$ 
    \begin{align*}
f_{X|y}(x)=\frac{f(x,y)}{f_Y(y)}.
\end{align*}
Es un función de densidad en $X$. Sus valores pueden depender (en el sentido de función matemática, de $y$, pero como este valor es dado, no representa un factor aleatorio). Análogamente, para cada valor $x$
	   \begin{align*}
	f_{Y|x}(y)=\frac{f(x,y)}{f_X(x)}.
	\end{align*}
    \end{thmd}
\end{framed} 

Entonces calculemos la función de densidad marginal de $Y$ en este problema es:
\begin{align*}
f_Y(y)=\int_{-\infty}^\infty f(x,y)dx=\int_{0}^\infty  \frac{1}{y}\exp \left(-\frac{x}{y}-y \right)dx=\frac{1}{y}\int_{0}^\infty \exp \left(-\frac{x}{y}-y \right)dx
\end{align*}
ocupando un cambio de variable $m=-\frac{x}{y}-y\Rightarrow dm=-\frac{1}{y}dx$, entonces
\begin{align*}
f_Y(y)&=\frac{1}{y}\int_{0}^\infty \exp \left(-\frac{x}{y}-y \right)dx=\frac{1}{y}\int_{0}^\infty \exp \left(m \right)(-y\ dm)=-\exp(m)=-\exp\left.\left(-\frac{x}{y}-y\right)\right|_{x=0}^\infty\\ \\ 
&=\exp(-y).
\end{align*}
Es decir, la marginal de $Y$ es:
$$f_Y(y)=\exp(-y), \ \ y>0.$$
Entonces la distribución conjunta de $X|Y=y$ es:
\begin{align*}
f_{X|y}(x)=\frac{f(x,y)}{f_Y(y)}=\frac{\frac{1}{y}\exp\left(-\frac{x}{y}-y \right) }{\exp(-y)}=\frac{1}{y}\exp\left(\frac{x}{y}-y +y\right)=\frac{1}{y}\exp\left(\frac{x}{y}\right).
\end{align*}
Por lo tanto:
\begin{align*}
\mE(X|Y=y) =\int_{-\infty}^\infty x f_{X|y}(x)dx=\int_{0}^\infty \frac{x}{y}\exp\left(\frac{x}{y}\right). dx.
\end{align*}
Si observamos, es la esperanza de una variable que se distribuye como una exponencial con parámetro $\frac{1}{y}.$ Entonces, como sabemos que la esperanza de una variable poisson con parámetro $\lambda$ es $\frac{1}{\lambda}.$ Entonces \textbf{podemos concluir que esperanza condicional es}:
\begin{align*}
\mE(X|Y=y)= y.\ \ \ \finf
\end{align*}

%Problema 4
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%

4. Sea $Y\sim \exp(\theta)$ y dado $Y=y$, X tiene distribución de Poisson de media $y$. Encuentre la ley de $X$.

\res Veamos que la densidad de $Y$ es 
\begin{align*}
\fy = \theta e^{-\theta y}. \ \ y>0.
\end{align*}
Y podemos ver que la densidad condicional de $X$ dado $Y=y$ es
\begin{align*}
f_{X|y}(x)= \frac{y^xe^{-y}}{x!}. \ x>0.
\end{align*}
Entonces usando la ley de probabilidad total tenemos que 
\begin{align*}
\fx&=\int_0^\infty f_{X|y}(x)f_Y(y)dy = \int_0^\infty \frac{y^xe^{-y}}{x!}  \theta e^{-\theta y} dy = \theta\int_0^\infty \frac{y^x e^{-(1+\theta)y}}{x!}dy
\end{align*}
haciendo un cambio de variable tenemos que $m=(1+\theta)y\Rightarrow dm=(1+\theta),$ entonces
\begin{align*}
\theta\int_0^\infty \frac{(m/(1+\theta))^x e^{-m}}{x!}dm= \frac{\theta}{(1+\theta)^{x+1}}\int_0^\infty \frac{m^x e^{-m}}{x!}dy=\frac{\theta}{(1+\theta)^{x+1}}.
\end{align*}
Por lo tanto, tenemos que
\begin{align*}
\fx = \frac{\theta}{(1+\theta)^{x+1}}, \ \ x=0,1\cdots
\end{align*}
Si hacemos un cambio de variable $p=\frac{1}{1+\theta}$, tenemos que lo anterior es igual a 
\begin{align*}
\fx = \frac{\theta}{(1+\theta)^{x+1}}=(1-p)p^n.
\end{align*}
Entonces podemos observar que $X\sim Geo(p).$ \ \ \ \ \fin

%Problema 5
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
5. Sea $(X, Y)$ un vector aleatorio con la siguiente densidad
\begin{align*}
f(x,y)=\left\{\begin{matrix}
\pi^{-1} & x^2+y^2\leq 1\\
0 & x^2+y^2>1
\end{matrix} \right.
\end{align*} 
Demuestre $X$ y $Y$ no están correlacionadas, pero que no son independientes.

\res Observemos que $X$ y $Y$ pueden tomar cualquier valor en el intervalo $[-1,1]$, por lo que podemos decir que son variables continuas. Entonces ocupando la definición de densidades marginales \ref{marginales}, las densidades marginales son:
\begin{align*}
\fy=\int_{-\sqrt{y^2-1}}^{\sqrt{y^2-1}}\pi^{-1} dx=\left.x\pi^{-1} \right|_{-\sqrt{y^2-1}}^{\sqrt{y^2-1}}=2\sqrt{y^2-1}\pi^{-1}, \ -1\leq y \leq 1.\\
\fx=\int_{-\sqrt{x^2-1}}^{\sqrt{x^2-1}}\pi^{-1} dy=\left.y\pi^{-1} \right|_{-\sqrt{x^2-1}}^{\sqrt{x^2-1}}=2\sqrt{x^2-1}\pi^{-1}, \ -1\leq x \leq 1,
\end{align*}
es decir, 
\begin{align*}
\fx=\left\{ \begin{array}{cc}
2\sqrt{x^2-1}\pi^{-1} & -1\leq x \leq 1.\\
0 & \text{en otra caso.}
\end{array} \right., \ \text{y}\ \fy=\left\{ \begin{array}{cc}
2\sqrt{y^2-1}\pi^{-1} & -1\leq y \leq 1.\\
0 & \text{en otra caso.}
\end{array} \right.
\end{align*}
Ocupando el teorema \ref{independencia} podemos observar que
\begin{align*}
f(x,y)=\pi^{-1}\neq 2\sqrt{x^2-1}\pi^{-1} 2\sqrt{y^2-1}\pi^{-1} =\fx \fy.
\end{align*}
\textbf{Por lo que podemos concluir que $X$ y $Y$ no son independientes.}\\ 

Ahora calculemos las siguientes esperanzas $\mE[X], \mE[Y], \mE[XY]:$
\begin{align*}
\mE[X]=\intim x \fx dx = \int_{-1}^1 2x\sqrt{x^2-1}\pi^{-1}dx
\end{align*}
ocupando un cambio de variable $m=x^2-1\Rightarrow dm =2xdx$, entonces 
\begin{align*}
\int 2x\sqrt{x^2-1}\pi^{-1}dx=\pi^{-1}\int \sqrt{m} dm=\pi^{-1}\frac{2m^{3/2}}{3} = \frac{2(x^2-1)^{3/2} \pi^{-1}}{3}.
\end{align*}
Por lo que 
\begin{align*}
E[X]=\left. \frac{2(x^2-1)^{3/2} \pi^{-1}}{3} \right|_{-1}^1=\frac{2(1^2-1)^{3/2} \pi^{-1}}{3}-\frac{2(1^2-1)^{3/2} \pi^{-1}}{3}=0.
\end{align*}
Ahora,
\begin{align*}
\mE[Y]=\intim y \fy dy = \int_{-1}^1 2y\sqrt{y^2-1}\pi^{-1}dy
\end{align*}
ocupando un cambio de variable $m=y^2-1\Rightarrow dm =2ydy$, entonces 
\begin{align*}
\int 2y\sqrt{y^2-1}\pi^{-1}dy=\pi^{-1}\int \sqrt{m} dm=\pi^{-1}\frac{2m^{3/2}}{3} = \frac{2(y^2-1)^{3/2} \pi^{-1}}{3}.
\end{align*}
Por lo que 
\begin{align*}
E[Y]=\left. \frac{2(y^2-1)^{3/2} \pi^{-1}}{3} \right|_{-1}^1=\frac{2(1^2-1)^{3/2} \pi^{-1}}{3}-\frac{2(1^2-1)^{3/2} \pi^{-1}}{3}=0.
\end{align*}
Y por último:
\begin{align*}
\mE[XY]=\intim\intim xy f(x,y) dx dy = \int_{-1}^1 \int_{-\sqrt{x^2-1}}^{\sqrt{x^2-1}} xy\pi^{-1}dydx=\int_{-1}^1 x\pi^{-1}\left( \int_{-\sqrt{x^2-1}}^{\sqrt{x^2-1}} y\right) dydx \\ \\
=\int_{-1}^1 x\pi^{-1}\left(\left. \frac{y^2}{2}\right|_{-\sqrt{x^2-1}}^{\sqrt{x^2-1}} \right) dx =\int_{-1}^1 x\pi^{-1}\left( \frac{{\sqrt{x^2-1}}^2-\sqrt{x^2-1}^2}{2} \right) dx \int_{-1}^1 x\pi^{-1}\left(0 \right) dx =0.
\end{align*}
Entonces ocupando los resultados anteriores tenemos que 
\begin{align*}
Cov(X,Y)=\mE[(X-\mE(X))(Y-\mE(Y))]=\mE(XY)-\mE(X)\mE(Y)=0-0=0.
\end{align*}
Lo anterior implica que $\rho=0.$ \textbf{Y por lo tanto, podemos concluir que las $X$ y $Y$ no están correlacionadas.} \ \ \ \fin

Este es un claro ejemplo de que si $Cov(X,Y)=0$, no necesariamente es cierto que las variables sean independientes, simplemente dice que no hay dependencia lineal, pero puede ser otro tipo de dependencia, que en el sentido contrario si es cierto. 
%Problema 6
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%

6. \textbf{Se entrega en la próxima tarea.} \\

%Problema 7
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%

%Problema 8
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------------------------------------%
8. Supongamos que se desea estudiar un número, $n$, grande de muestras de sangre para tratar de detectar una enfermedad rara en cada una de las muestras. Si cada muestra es analizada individualmente, se requieren $n$ pruebas. Supongamos que cada muestra se divide a la mitad y que de cada muestra se selecciona una de las mitades, las que se usan para crear una mezcla de todas ellas (i.e. se juntan en una sola muestra). Si la prueba es suficientemente sensitiva, esta mezcla puede examinarse y, si tal prueba sale negativa, no se requieren hacer mías pruebas y solo se necesita una (no hay enfermos en la muestra). Si la prueba de la mezcla es positiva, cada una de las mitades reservadas de la muestras (las mitades no mezcladas) pueden ser analizadas individualmente. En dicho caso, se necesitan un total de $n+ 1$. Así, si la enfermedad es rara, es posible que se puedan lograr algunos ahorros a través de este método
de analizar en grupo.\\
Generalicemos el esquema anterior y supongamos que las $n$ muestras primero se agrupan en $m$ grupos de tamaño $k$ (muestras); es decir, $n = mk$. Cada grupo se prueba con el método de grupo que se explico en el párrafo anterior; si un grupo sale positivo, cada individuo en el grupo es analizado individualmente. Si $X_i$ es el número de pruebas que se hacen en el $i-$ésimo grupo, el número total de pruebas que se hacen es $N =\sum_{i=1}^m Xi$. Sea $p$ la probabilidad de
que un individuo cualquiera salga negativo en la prueba. Calcule la esperanza de $N$. ¿Por qué es importante que la enfermedad sea rara? Hint: Lea cuidadosamente el ejercicio. Note que la prueba de una muestra resulta en una v.a. $Bernoulli(p)$.

\res Veamos como se distribuye $X_i,$ solo puede tomar dos valores 1 o $k+1$. La probabilidad de que $X_i$ sea 1 es $p^k$, ya que es la probabilidad de la todos los $k$ individuos en el grupo $i$ salen negativos y sabiendo que $p$ es la probabilidad de que un individuo cualquiera salga negativo, y como la probabilidad de que un individuo salga negativo es independiente a que otro individuo salga negativo se concluye que es $p^k$. Ahora, La probabilidad de que $X_i$ sea $k+1$ es $1-p^k$, ya que es la probabilidad de que salgan algún positivo en el grupo $i$ pero es equivalente al complemento de que todos salgan negativos. Por lo tanto, tenemos que la función de densidad de $X_i$ es:
\begin{align*}
f(x_i)=\left\{\begin{array}{cc}
p^k & x_i=1\\
1-p^k & x_i=k+1. 
\end{array} \right.
\end{align*}
Ahora, por como esta definidos los $X_i$ suponemos que son independientes. Entonces tenemos que
\begin{align*}
\mE[N]&=\mE\left[\sum_{i=1}^m Xi\right]= \sum_{i=1}^m \mE[X_i]=\sum_{i=1}^m 1\cdot p^k+(1-p^k)*(k+1)=\sum_{i=1}^m p^k+k+1-kp^k-p^k\\
&=\sum_{i=1}^m (k+1-kp^k)=m(k+1-kp^k).
\end{align*}
Si la enfermedad es rara eso implica que $p^k$ sea pequeña, por lo que en teoría $p^k>1-p^k$, es decir, sería más probable hacer una prueba que $k+1$ pruebas. Entonces en términos económicos y de tiempos, la metodología expuesta es muy eficaz y más óptima que analizar siempre todas las muestras por separados.  \ \ \ \fin

\textbf{Honours problems}

1. Sea $X$ una v.a continua con segundo momento finito. Demuestra que la mediana $M(X)$ de $X$ satisface

\begin{align*}
|M(X)-E(X)|\leq (Var(X))^{1/2}.
\end{align*}

Hint: Use los honour problems de la tarea anterior.

\res Recordemos el siguiente teorema (visto en clase). 
\begin{framed}
    \begin{thmt}(Desigualdad de Jensen) \label{desigualdad_de_Jensen}
    Si $X$ es una v.a con primer momento finito y $g$ es convexa, entonces 
    $$E(g(X))\geq g(E(X)).$$
    \end{thmt}
\end{framed} 

Si definimos a $g(x)=(|x-c|)^2\Leftrightarrow g(x)=(x-c)^2,$  observemos que:
\begin{align*}
g'(x)=2x-2c \ \ \text{y} \ \ g''(x)=2.
\end{align*}
Entonces como $g''(x)>0$ podemos decir que $g(x)$ es convexa. Ocupando el teorema \ref{desigualdad_de_Jensen} tenemos que 
\begin{align} \label{desigualdad}
\mE(|X-c|^2)\geq |c-\mE(X)|^2.
\end{align}
Observemos que cuando $c=\mE(X)$
\begin{align*}
E(|x-\mE(X)|^2)=Var(X).
\end{align*}
Ocupando lo demostrado en la tarea 3, podemos decir que cuando $c=M(X)$ se minimiza $\mE(|X-c|^2),$ por lo que se cumple que
\begin{align*}
Var(X)=\mE(|x-\mE(X)|^2)\geq \mE(|X-M(X)|^2)
\end{align*}
Y por lo tanto, ocupando \ref{desigualdad}:
\begin{align*}
Var(X)=\mE(|x-\mE|^2)\geq \mE(|X-M(X)|^2) &\geq (|M(X)-\mE(X)|^2)\ \Rightarrow \\ \\
(Var(X))^{1/2}&\geq |M(X)-\mE(X)|\ \ \ \finf
\end{align*}

\textbf{Ejercicios de las notas:}

1. Se quiere verificar la programación del tiempo que debe durar la luz verde en un semáforo que se encuentra en una cierta intersección, con vuelta a la izquierda. Como no se tiene información al respecto, se envía a un estudiante graduado a hacer observaciones sobre el número de carros $X$
y el número de camiones $Y$ que llegan en un ciclo (entre verde y verde); y con esto se construye la tabla de distribuciones conjunta. Se plantean una serie de preguntas y con ello confirmas qué tan eficiente fue el ciclo planeado. Los resultados fueron los siguientes
\begin{figure}[H]
\centering
\includegraphics[scale=1]{semaforo.png}
\end{figure}
\begin{enumerate}
\item Verifica que es una tabla válida de probabilidades conjuntas.
\item Calcula las distribuciones marginales para carros y camiones.
\item ¿Cuál es la probabilidad de que se tengan exactamente un carro y un camión en un ciclo dado?
\item Supongamos que la vuelta a la izquierda tiene una capacidad máxima de 5 carros y un camión es equivalente a 3 carros. ¿ Cuál es la probabilidad de que se sature la línea en un ciclo dado?
\item  ¿Cuál es la probabilidad de que se tenga exactamente un carro, dado que ya se tiene un camión en un ciclo predeterminado y se sabe que no habrá ningún otro camión más? Contesta esta misma pregunta para $Y = 0, 1, 2, 3, 4, 5$.
\end{enumerate}

2. Calcula la regresión de Y 1 sobre Y 2 = y 2 = 1 en el ejemplo. Tendría ésta un significado valorable? explica tanto como puedas.

3. En un área determinada, cierto material es seleccionado al azar y pesado en dos tiempos distintos. Sea $W = $peso del material (verdadero) y $X_1$ , $X_2$ las dos mediciones hechas. Uno puede pensar estas mediciones como
$$X_1 =W + e_1$$
$$X_2 =W + e_2$$,
donde $e_1$ y $e_2$ son los errores de medición. Supongamos que $e_1$ y $e_2$ son independientes entre sí e independientes de $W$ y que $V(e_1)=V(e_2)=\sigma_e^2$. Expresar a $\rho$, el coeficiente de correlación entre $X_1$ y $X_2$ , en términos de $\bf\sigma^2$ (la varianza del peso verdadero) y $\sigma^2$ ; interpreta el resultado.

4. Para $t=(t_1, t_2)$, la función generatriz de momentos conjunta del vector $W=(X,Y)$ está dada por 
\begin{align*}
M_{X,Y}(t_1, t_2)&= \mE(e^{t'Q})=\mE(e^{t_1x+t_2y})\\
&=\exp\left\{t_1\mu_x+t_2\mu_y+\frac{1}{2}(\sigma_x^2t_1^2+2\rho \sigma_x\sigma_y t_1t_2+\sigma_y^2t_2^2)\right\}
\end{align*}

5. Sean $X$ y $Y$ v.a. Mostrar que $\mE(Y|X)$ es el mejor predictor de $Y$ entre todas las funciones de $X$.
\end{document}